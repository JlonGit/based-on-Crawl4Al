{
  "https://docs.crawl4ai.com/": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) ðð¤ Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1.âGenerate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2.âStructured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3.âAdvanced Browser Control: Hooks, proxies, stealth modes, session re-useâfine-grained control. 4.âHigh Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5.âOpen Source: No forced API keys, no paywallsâeveryone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, weâve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, youâll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether itâs a small fix, a big feature, or better docsâcontributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyoneâstudents, researchers, entrepreneurs, data scientistsâto access, parse, and shape the worldâs data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Letâs keep building an open, democratic approach to data extraction and AI together. Happy Crawling! â Unclecode, Founder & Maintainer of Crawl4AI ðð¤ Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1.âGenerate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2.âStructured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3.âAdvanced Browser Control: Hooks, proxies, stealth modes, session re-useâfine-grained control. 4.âHigh Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5.âOpen Source: No forced API keys, no paywallsâeveryone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, weâve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, youâll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether itâs a small fix, a big feature, or better docsâcontributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyoneâstudents, researchers, entrepreneurs, data scientistsâto access, parse, and shape the worldâs data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Letâs keep building an open, democratic approach to data extraction and AI together. Happy Crawling! â Unclecode, Founder & Maintainer of Crawl4AI ðð¤ Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1.âGenerate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2.âStructured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3.âAdvanced Browser Control: Hooks, proxies, stealth modes, session re-useâfine-grained control. 4.âHigh Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5.âOpen Source: No forced API keys, no paywallsâeveryone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, weâve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, youâll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether itâs a small fix, a big feature, or better docsâcontributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyoneâstudents, researchers, entrepreneurs, data scientistsâto access, parse, and shape the worldâs data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Letâs keep building an open, democratic approach to data extraction and AI together. Happy Crawling! â Unclecode, Founder & Maintainer of Crawl4AI ðð¤ Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1.âGenerate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2.âStructured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3.âAdvanced Browser Control: Hooks, proxies, stealth modes, session re-useâfine-grained control. 4.âHigh Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5.âOpen Source: No forced API keys, no paywallsâeveryone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, weâve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, youâll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether itâs a small fix, a big feature, or better docsâcontributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyoneâstudents, researchers, entrepreneurs, data scientistsâto access, parse, and shape the worldâs data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Letâs keep building an open, democratic approach to data extraction and AI together. Happy Crawling! â Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/core/quickstart/": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Getting Started with Crawl4AI Welcome to Crawl4AI, an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you’ll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how it’s influenced by content filters). Experiment with a simple CSS-based extraction strategy. See a glimpse of LLM-based extraction (including open-source and closed-source model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler. Configurable browser and run settings via BrowserConfig and CrawlerRunConfig. Automatic HTML-to-Markdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLM-based or “traditional” CSS/XPath-based). By the end of this guide, you’ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses “Load More” buttons or JavaScript updates. 2. Your First Crawl Here’s a minimal Python script that creates an AsyncWebCrawler, fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com\") print(result.markdown[:300]) # Print first 300 chars if __name__ == \"__main__\": asyncio.run(main()) What’s happening? - AsyncWebCrawler launches a headless browser (Chromium by default). - It fetches https://example.com. - Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) Crawl4AI’s crawler can be heavily customized using two main classes: 1. BrowserConfig: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2. CrawlerRunConfig: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browser_conf = BrowserConfig(headless=True) # or False to see the browser run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS ) async with AsyncWebCrawler(config=browser_conf) as crawler: result = await crawler.arun( url=\"https://example.com\", config=run_conf ) print(result.markdown) if __name__ == \"__main__\": asyncio.run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED. So to have fresh content, you need to set it to CacheMode.BYPASS We’ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter. result.markdown: The direct HTML-to-Markdown conversion. result.markdown.fit_markdown: The same content after applying any configured content filter (e.g., PruningContentFilter). Example: Using a Filter with DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_filter_strategy import PruningContentFilter from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator md_generator = DefaultMarkdownGenerator( content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\") ) config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, markdown_generator=md_generator ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://news.ycombinator.com\", config=config) print(\"Raw Markdown length:\", len(result.markdown.raw_markdown)) print(\"Fit Markdown length:\", len(result.markdown.fit_markdown)) Note: If you do not specify a content filter or markdown generator, you’ll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. We’ll dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSS-based) Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions: from crawl4ai.extraction_strategy import JsonCssExtractionStrategy # Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" # Using OpenAI (requires API token) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"openai/gpt-4o\", # Default provider api_token=\"your-openai-token\" # Required for OpenAI ) # Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"ollama/llama3.3\", # Open source alternative api_token=None # Not needed for Ollama ) # Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see No-LLM Extraction Strategies. Here's a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def main(): schema = { \"name\": \"Example Items\", \"baseSelector\": \"div.item\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"} ] } raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"raw://\" + raw_html, config=CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema) ) ) # The JSON output is stored in 'extracted_content' data = json.loads(result.extracted_content) print(data) if __name__ == \"__main__\": asyncio.run(main()) Why is this helpful? - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw://. 6. Simple Data Extraction (LLM-based) For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports open-source or closed-source providers: Open-Source Models (e.g., ollama/llama3.3, no_token) OpenAI Models (e.g., openai/gpt-4, requires api_token) Or any provider supported by the underlying library Below is an example using open-source style (no token) and closed-source: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field( ..., description=\"Fee for output token for the OpenAI model.\" ) async def extract_structured_data_using_llm( provider: str, api_token: str = None, extra_headers: Dict[str, str] = None ): print(f\"\\n--- Extracting Structured Data with {provider} ---\") if api_token is None and provider != \"ollama\": print(f\"API token is required for {provider}. Skipping this example.\") return browser_config = BrowserConfig(headless=True) extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000} if extra_headers: extra_args[\"extra_headers\"] = extra_headers crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, word_count_threshold=1, page_timeout=80000, extraction_strategy=LLMExtractionStrategy( provider=provider, api_token=api_token, schema=OpenAIModelFee.model_json_schema(), extraction_type=\"schema\", instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\", extra_args=extra_args, ), ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://openai.com/api/pricing/\", config=crawler_config ) print(result.extracted_content) if __name__ == \"__main__\": # Use ollama with llama3.3 # asyncio.run( # extract_structured_data_using_llm( # provider=\"ollama/llama3.3\", api_token=\"no-token\" # ) # ) asyncio.run( extract_structured_data_using_llm( provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\") ) ) What’s happening? - We define a Pydantic schema (PricingInfo) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the provider and api_token, you can use local models or a remote API. 7. Multi-URL Concurrency (Preview) If you need to crawl multiple URLs in parallel, you can use arun_many(). By default, Crawl4AI employs a MemoryAdaptiveDispatcher, automatically adjusting concurrency based on system resources. Here’s a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quick_parallel_example(): urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, stream=True # Enable streaming mode ) async with AsyncWebCrawler() as crawler: # Stream results as they complete async for result in await crawler.arun_many(urls, config=run_conf): if result.success: print(f\"[OK] {result.url}, length: {len(result.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {result.url} => {result.error_message}\") # Or get all results at once (default behavior) run_conf = run_conf.clone(stream=False) results = await crawler.arun_many(urls, config=run_conf) for res in results: if res.success: print(f\"[OK] {res.url}, length: {len(res.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {res.url} => {res.error_message}\") if __name__ == \"__main__\": asyncio.run(quick_parallel_example()) The example above shows two ways to handle multiple URLs: 1. Streaming mode (stream=True): Process results as they become available using async for 2. Batch mode (stream=False): Wait for all results to complete For more advanced concurrency (e.g., a semaphore-based approach, adaptive memory usage throttling, or customized rate limiting), see Advanced Multi-URL Crawling. 8. Dynamic Content Example Some sites require multiple “page clicks” or dynamic JavaScript updates. Below is an example showing how to click a “Next Page” button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def extract_structured_data_using_css_extractor(): print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\") schema = { \"name\": \"KidoCode Courses\", \"baseSelector\": \"section.charge-methodology .w-tab-content > div\", \"fields\": [ { \"name\": \"section_title\", \"selector\": \"h3.heading-50\", \"type\": \"text\", }, { \"name\": \"section_description\", \"selector\": \".charge-content\", \"type\": \"text\", }, { \"name\": \"course_name\", \"selector\": \".text-block-93\", \"type\": \"text\", }, { \"name\": \"course_description\", \"selector\": \".course-content-text\", \"type\": \"text\", }, { \"name\": \"course_icon\", \"selector\": \".image-92\", \"type\": \"attribute\", \"attribute\": \"src\", }, ], } browser_config = BrowserConfig(headless=True, java_script_enabled=True) js_click_tabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), js_code=[js_click_tabs], ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config ) companies = json.loads(result.extracted_content) print(f\"Successfully extracted {len(companies)} companies\") print(json.dumps(companies[0], indent=2)) async def main(): await extract_structured_data_using_css_extractor() if __name__ == \"__main__\": asyncio.run(main()) Key Points: BrowserConfig(headless=False): We want to watch it click “Next Page.” CrawlerRunConfig(...): We specify the extraction strategy, pass session_id to reuse the same page. js_code and wait_for are used for subsequent pages (page > 0) to click the “Next” button and wait for new commits to load. js_only=True indicates we’re not re-navigating but continuing the existing session. Finally, we call kill_session() to clean up the page and browser session. 9. Next Steps Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If you’re ready for more, check out: Installation: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks & Auth: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management: Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling! Getting Started with Crawl4AI Welcome to Crawl4AI, an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you’ll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how it’s influenced by content filters). Experiment with a simple CSS-based extraction strategy. See a glimpse of LLM-based extraction (including open-source and closed-source model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler. Configurable browser and run settings via BrowserConfig and CrawlerRunConfig. Automatic HTML-to-Markdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLM-based or “traditional” CSS/XPath-based). By the end of this guide, you’ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses “Load More” buttons or JavaScript updates. 2. Your First Crawl Here’s a minimal Python script that creates an AsyncWebCrawler, fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com\") print(result.markdown[:300]) # Print first 300 chars if __name__ == \"__main__\": asyncio.run(main()) What’s happening? - AsyncWebCrawler launches a headless browser (Chromium by default). - It fetches https://example.com. - Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) Crawl4AI’s crawler can be heavily customized using two main classes: 1. BrowserConfig: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2. CrawlerRunConfig: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browser_conf = BrowserConfig(headless=True) # or False to see the browser run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS ) async with AsyncWebCrawler(config=browser_conf) as crawler: result = await crawler.arun( url=\"https://example.com\", config=run_conf ) print(result.markdown) if __name__ == \"__main__\": asyncio.run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED. So to have fresh content, you need to set it to CacheMode.BYPASS We’ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter. result.markdown: The direct HTML-to-Markdown conversion. result.markdown.fit_markdown: The same content after applying any configured content filter (e.g., PruningContentFilter). Example: Using a Filter with DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_filter_strategy import PruningContentFilter from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator md_generator = DefaultMarkdownGenerator( content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\") ) config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, markdown_generator=md_generator ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://news.ycombinator.com\", config=config) print(\"Raw Markdown length:\", len(result.markdown.raw_markdown)) print(\"Fit Markdown length:\", len(result.markdown.fit_markdown)) Note: If you do not specify a content filter or markdown generator, you’ll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. We’ll dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSS-based) Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions: from crawl4ai.extraction_strategy import JsonCssExtractionStrategy # Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" # Using OpenAI (requires API token) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"openai/gpt-4o\", # Default provider api_token=\"your-openai-token\" # Required for OpenAI ) # Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"ollama/llama3.3\", # Open source alternative api_token=None # Not needed for Ollama ) # Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see No-LLM Extraction Strategies. Here's a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def main(): schema = { \"name\": \"Example Items\", \"baseSelector\": \"div.item\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"} ] } raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"raw://\" + raw_html, config=CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema) ) ) # The JSON output is stored in 'extracted_content' data = json.loads(result.extracted_content) print(data) if __name__ == \"__main__\": asyncio.run(main()) Why is this helpful? - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw://. 6. Simple Data Extraction (LLM-based) For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports open-source or closed-source providers: Open-Source Models (e.g., ollama/llama3.3, no_token) OpenAI Models (e.g., openai/gpt-4, requires api_token) Or any provider supported by the underlying library Below is an example using open-source style (no token) and closed-source: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field( ..., description=\"Fee for output token for the OpenAI model.\" ) async def extract_structured_data_using_llm( provider: str, api_token: str = None, extra_headers: Dict[str, str] = None ): print(f\"\\n--- Extracting Structured Data with {provider} ---\") if api_token is None and provider != \"ollama\": print(f\"API token is required for {provider}. Skipping this example.\") return browser_config = BrowserConfig(headless=True) extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000} if extra_headers: extra_args[\"extra_headers\"] = extra_headers crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, word_count_threshold=1, page_timeout=80000, extraction_strategy=LLMExtractionStrategy( provider=provider, api_token=api_token, schema=OpenAIModelFee.model_json_schema(), extraction_type=\"schema\", instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\", extra_args=extra_args, ), ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://openai.com/api/pricing/\", config=crawler_config ) print(result.extracted_content) if __name__ == \"__main__\": # Use ollama with llama3.3 # asyncio.run( # extract_structured_data_using_llm( # provider=\"ollama/llama3.3\", api_token=\"no-token\" # ) # ) asyncio.run( extract_structured_data_using_llm( provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\") ) ) What’s happening? - We define a Pydantic schema (PricingInfo) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the provider and api_token, you can use local models or a remote API. 7. Multi-URL Concurrency (Preview) If you need to crawl multiple URLs in parallel, you can use arun_many(). By default, Crawl4AI employs a MemoryAdaptiveDispatcher, automatically adjusting concurrency based on system resources. Here’s a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quick_parallel_example(): urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, stream=True # Enable streaming mode ) async with AsyncWebCrawler() as crawler: # Stream results as they complete async for result in await crawler.arun_many(urls, config=run_conf): if result.success: print(f\"[OK] {result.url}, length: {len(result.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {result.url} => {result.error_message}\") # Or get all results at once (default behavior) run_conf = run_conf.clone(stream=False) results = await crawler.arun_many(urls, config=run_conf) for res in results: if res.success: print(f\"[OK] {res.url}, length: {len(res.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {res.url} => {res.error_message}\") if __name__ == \"__main__\": asyncio.run(quick_parallel_example()) The example above shows two ways to handle multiple URLs: 1. Streaming mode (stream=True): Process results as they become available using async for 2. Batch mode (stream=False): Wait for all results to complete For more advanced concurrency (e.g., a semaphore-based approach, adaptive memory usage throttling, or customized rate limiting), see Advanced Multi-URL Crawling. 8. Dynamic Content Example Some sites require multiple “page clicks” or dynamic JavaScript updates. Below is an example showing how to click a “Next Page” button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def extract_structured_data_using_css_extractor(): print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\") schema = { \"name\": \"KidoCode Courses\", \"baseSelector\": \"section.charge-methodology .w-tab-content > div\", \"fields\": [ { \"name\": \"section_title\", \"selector\": \"h3.heading-50\", \"type\": \"text\", }, { \"name\": \"section_description\", \"selector\": \".charge-content\", \"type\": \"text\", }, { \"name\": \"course_name\", \"selector\": \".text-block-93\", \"type\": \"text\", }, { \"name\": \"course_description\", \"selector\": \".course-content-text\", \"type\": \"text\", }, { \"name\": \"course_icon\", \"selector\": \".image-92\", \"type\": \"attribute\", \"attribute\": \"src\", }, ], } browser_config = BrowserConfig(headless=True, java_script_enabled=True) js_click_tabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), js_code=[js_click_tabs], ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config ) companies = json.loads(result.extracted_content) print(f\"Successfully extracted {len(companies)} companies\") print(json.dumps(companies[0], indent=2)) async def main(): await extract_structured_data_using_css_extractor() if __name__ == \"__main__\": asyncio.run(main()) Key Points: BrowserConfig(headless=False): We want to watch it click “Next Page.” CrawlerRunConfig(...): We specify the extraction strategy, pass session_id to reuse the same page. js_code and wait_for are used for subsequent pages (page > 0) to click the “Next” button and wait for new commits to load. js_only=True indicates we’re not re-navigating but continuing the existing session. Finally, we call kill_session() to clean up the page and browser session. 9. Next Steps Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If you’re ready for more, check out: Installation: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks & Auth: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management: Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling! Getting Started with Crawl4AI Welcome to Crawl4AI, an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you’ll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how it’s influenced by content filters). Experiment with a simple CSS-based extraction strategy. See a glimpse of LLM-based extraction (including open-source and closed-source model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler. Configurable browser and run settings via BrowserConfig and CrawlerRunConfig. Automatic HTML-to-Markdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLM-based or “traditional” CSS/XPath-based). By the end of this guide, you’ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses “Load More” buttons or JavaScript updates. 2. Your First Crawl Here’s a minimal Python script that creates an AsyncWebCrawler, fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com\") print(result.markdown[:300]) # Print first 300 chars if __name__ == \"__main__\": asyncio.run(main()) What’s happening? - AsyncWebCrawler launches a headless browser (Chromium by default). - It fetches https://example.com. - Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) Crawl4AI’s crawler can be heavily customized using two main classes: 1. BrowserConfig: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2. CrawlerRunConfig: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browser_conf = BrowserConfig(headless=True) # or False to see the browser run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS ) async with AsyncWebCrawler(config=browser_conf) as crawler: result = await crawler.arun( url=\"https://example.com\", config=run_conf ) print(result.markdown) if __name__ == \"__main__\": asyncio.run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED. So to have fresh content, you need to set it to CacheMode.BYPASS We’ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter. result.markdown: The direct HTML-to-Markdown conversion. result.markdown.fit_markdown: The same content after applying any configured content filter (e.g., PruningContentFilter). Example: Using a Filter with DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_filter_strategy import PruningContentFilter from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator md_generator = DefaultMarkdownGenerator( content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\") ) config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, markdown_generator=md_generator ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://news.ycombinator.com\", config=config) print(\"Raw Markdown length:\", len(result.markdown.raw_markdown)) print(\"Fit Markdown length:\", len(result.markdown.fit_markdown)) Note: If you do not specify a content filter or markdown generator, you’ll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. We’ll dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSS-based) Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions: from crawl4ai.extraction_strategy import JsonCssExtractionStrategy # Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" # Using OpenAI (requires API token) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"openai/gpt-4o\", # Default provider api_token=\"your-openai-token\" # Required for OpenAI ) # Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"ollama/llama3.3\", # Open source alternative api_token=None # Not needed for Ollama ) # Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see No-LLM Extraction Strategies. Here's a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def main(): schema = { \"name\": \"Example Items\", \"baseSelector\": \"div.item\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"} ] } raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"raw://\" + raw_html, config=CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema) ) ) # The JSON output is stored in 'extracted_content' data = json.loads(result.extracted_content) print(data) if __name__ == \"__main__\": asyncio.run(main()) Why is this helpful? - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw://. 6. Simple Data Extraction (LLM-based) For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports open-source or closed-source providers: Open-Source Models (e.g., ollama/llama3.3, no_token) OpenAI Models (e.g., openai/gpt-4, requires api_token) Or any provider supported by the underlying library Below is an example using open-source style (no token) and closed-source: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field( ..., description=\"Fee for output token for the OpenAI model.\" ) async def extract_structured_data_using_llm( provider: str, api_token: str = None, extra_headers: Dict[str, str] = None ): print(f\"\\n--- Extracting Structured Data with {provider} ---\") if api_token is None and provider != \"ollama\": print(f\"API token is required for {provider}. Skipping this example.\") return browser_config = BrowserConfig(headless=True) extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000} if extra_headers: extra_args[\"extra_headers\"] = extra_headers crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, word_count_threshold=1, page_timeout=80000, extraction_strategy=LLMExtractionStrategy( provider=provider, api_token=api_token, schema=OpenAIModelFee.model_json_schema(), extraction_type=\"schema\", instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\", extra_args=extra_args, ), ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://openai.com/api/pricing/\", config=crawler_config ) print(result.extracted_content) if __name__ == \"__main__\": # Use ollama with llama3.3 # asyncio.run( # extract_structured_data_using_llm( # provider=\"ollama/llama3.3\", api_token=\"no-token\" # ) # ) asyncio.run( extract_structured_data_using_llm( provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\") ) ) What’s happening? - We define a Pydantic schema (PricingInfo) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the provider and api_token, you can use local models or a remote API. 7. Multi-URL Concurrency (Preview) If you need to crawl multiple URLs in parallel, you can use arun_many(). By default, Crawl4AI employs a MemoryAdaptiveDispatcher, automatically adjusting concurrency based on system resources. Here’s a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quick_parallel_example(): urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, stream=True # Enable streaming mode ) async with AsyncWebCrawler() as crawler: # Stream results as they complete async for result in await crawler.arun_many(urls, config=run_conf): if result.success: print(f\"[OK] {result.url}, length: {len(result.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {result.url} => {result.error_message}\") # Or get all results at once (default behavior) run_conf = run_conf.clone(stream=False) results = await crawler.arun_many(urls, config=run_conf) for res in results: if res.success: print(f\"[OK] {res.url}, length: {len(res.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {res.url} => {res.error_message}\") if __name__ == \"__main__\": asyncio.run(quick_parallel_example()) The example above shows two ways to handle multiple URLs: 1. Streaming mode (stream=True): Process results as they become available using async for 2. Batch mode (stream=False): Wait for all results to complete For more advanced concurrency (e.g., a semaphore-based approach, adaptive memory usage throttling, or customized rate limiting), see Advanced Multi-URL Crawling. 8. Dynamic Content Example Some sites require multiple “page clicks” or dynamic JavaScript updates. Below is an example showing how to click a “Next Page” button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def extract_structured_data_using_css_extractor(): print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\") schema = { \"name\": \"KidoCode Courses\", \"baseSelector\": \"section.charge-methodology .w-tab-content > div\", \"fields\": [ { \"name\": \"section_title\", \"selector\": \"h3.heading-50\", \"type\": \"text\", }, { \"name\": \"section_description\", \"selector\": \".charge-content\", \"type\": \"text\", }, { \"name\": \"course_name\", \"selector\": \".text-block-93\", \"type\": \"text\", }, { \"name\": \"course_description\", \"selector\": \".course-content-text\", \"type\": \"text\", }, { \"name\": \"course_icon\", \"selector\": \".image-92\", \"type\": \"attribute\", \"attribute\": \"src\", }, ], } browser_config = BrowserConfig(headless=True, java_script_enabled=True) js_click_tabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), js_code=[js_click_tabs], ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config ) companies = json.loads(result.extracted_content) print(f\"Successfully extracted {len(companies)} companies\") print(json.dumps(companies[0], indent=2)) async def main(): await extract_structured_data_using_css_extractor() if __name__ == \"__main__\": asyncio.run(main()) Key Points: BrowserConfig(headless=False): We want to watch it click “Next Page.” CrawlerRunConfig(...): We specify the extraction strategy, pass session_id to reuse the same page. js_code and wait_for are used for subsequent pages (page > 0) to click the “Next” button and wait for new commits to load. js_only=True indicates we’re not re-navigating but continuing the existing session. Finally, we call kill_session() to clean up the page and browser session. 9. Next Steps Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If you’re ready for more, check out: Installation: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks & Auth: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management: Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling! Getting Started with Crawl4AI Welcome to Crawl4AI, an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you’ll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how it’s influenced by content filters). Experiment with a simple CSS-based extraction strategy. See a glimpse of LLM-based extraction (including open-source and closed-source model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler. Configurable browser and run settings via BrowserConfig and CrawlerRunConfig. Automatic HTML-to-Markdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLM-based or “traditional” CSS/XPath-based). By the end of this guide, you’ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses “Load More” buttons or JavaScript updates. 2. Your First Crawl Here’s a minimal Python script that creates an AsyncWebCrawler, fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com\") print(result.markdown[:300]) # Print first 300 chars if __name__ == \"__main__\": asyncio.run(main()) What’s happening? - AsyncWebCrawler launches a headless browser (Chromium by default). - It fetches https://example.com. - Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) Crawl4AI’s crawler can be heavily customized using two main classes: 1. BrowserConfig: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2. CrawlerRunConfig: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browser_conf = BrowserConfig(headless=True) # or False to see the browser run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS ) async with AsyncWebCrawler(config=browser_conf) as crawler: result = await crawler.arun( url=\"https://example.com\", config=run_conf ) print(result.markdown) if __name__ == \"__main__\": asyncio.run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED. So to have fresh content, you need to set it to CacheMode.BYPASS We’ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter. result.markdown: The direct HTML-to-Markdown conversion. result.markdown.fit_markdown: The same content after applying any configured content filter (e.g., PruningContentFilter). Example: Using a Filter with DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_filter_strategy import PruningContentFilter from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator md_generator = DefaultMarkdownGenerator( content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\") ) config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, markdown_generator=md_generator ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://news.ycombinator.com\", config=config) print(\"Raw Markdown length:\", len(result.markdown.raw_markdown)) print(\"Fit Markdown length:\", len(result.markdown.fit_markdown)) Note: If you do not specify a content filter or markdown generator, you’ll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. We’ll dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSS-based) Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions: from crawl4ai.extraction_strategy import JsonCssExtractionStrategy # Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" # Using OpenAI (requires API token) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"openai/gpt-4o\", # Default provider api_token=\"your-openai-token\" # Required for OpenAI ) # Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"ollama/llama3.3\", # Open source alternative api_token=None # Not needed for Ollama ) # Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see No-LLM Extraction Strategies. Here's a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def main(): schema = { \"name\": \"Example Items\", \"baseSelector\": \"div.item\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"} ] } raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"raw://\" + raw_html, config=CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema) ) ) # The JSON output is stored in 'extracted_content' data = json.loads(result.extracted_content) print(data) if __name__ == \"__main__\": asyncio.run(main()) Why is this helpful? - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw://. 6. Simple Data Extraction (LLM-based) For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports open-source or closed-source providers: Open-Source Models (e.g., ollama/llama3.3, no_token) OpenAI Models (e.g., openai/gpt-4, requires api_token) Or any provider supported by the underlying library Below is an example using open-source style (no token) and closed-source: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field( ..., description=\"Fee for output token for the OpenAI model.\" ) async def extract_structured_data_using_llm( provider: str, api_token: str = None, extra_headers: Dict[str, str] = None ): print(f\"\\n--- Extracting Structured Data with {provider} ---\") if api_token is None and provider != \"ollama\": print(f\"API token is required for {provider}. Skipping this example.\") return browser_config = BrowserConfig(headless=True) extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000} if extra_headers: extra_args[\"extra_headers\"] = extra_headers crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, word_count_threshold=1, page_timeout=80000, extraction_strategy=LLMExtractionStrategy( provider=provider, api_token=api_token, schema=OpenAIModelFee.model_json_schema(), extraction_type=\"schema\", instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\", extra_args=extra_args, ), ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://openai.com/api/pricing/\", config=crawler_config ) print(result.extracted_content) if __name__ == \"__main__\": # Use ollama with llama3.3 # asyncio.run( # extract_structured_data_using_llm( # provider=\"ollama/llama3.3\", api_token=\"no-token\" # ) # ) asyncio.run( extract_structured_data_using_llm( provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\") ) ) What’s happening? - We define a Pydantic schema (PricingInfo) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the provider and api_token, you can use local models or a remote API. 7. Multi-URL Concurrency (Preview) If you need to crawl multiple URLs in parallel, you can use arun_many(). By default, Crawl4AI employs a MemoryAdaptiveDispatcher, automatically adjusting concurrency based on system resources. Here’s a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quick_parallel_example(): urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, stream=True # Enable streaming mode ) async with AsyncWebCrawler() as crawler: # Stream results as they complete async for result in await crawler.arun_many(urls, config=run_conf): if result.success: print(f\"[OK] {result.url}, length: {len(result.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {result.url} => {result.error_message}\") # Or get all results at once (default behavior) run_conf = run_conf.clone(stream=False) results = await crawler.arun_many(urls, config=run_conf) for res in results: if res.success: print(f\"[OK] {res.url}, length: {len(res.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {res.url} => {res.error_message}\") if __name__ == \"__main__\": asyncio.run(quick_parallel_example()) The example above shows two ways to handle multiple URLs: 1. Streaming mode (stream=True): Process results as they become available using async for 2. Batch mode (stream=False): Wait for all results to complete For more advanced concurrency (e.g., a semaphore-based approach, adaptive memory usage throttling, or customized rate limiting), see Advanced Multi-URL Crawling. 8. Dynamic Content Example Some sites require multiple “page clicks” or dynamic JavaScript updates. Below is an example showing how to click a “Next Page” button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def extract_structured_data_using_css_extractor(): print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\") schema = { \"name\": \"KidoCode Courses\", \"baseSelector\": \"section.charge-methodology .w-tab-content > div\", \"fields\": [ { \"name\": \"section_title\", \"selector\": \"h3.heading-50\", \"type\": \"text\", }, { \"name\": \"section_description\", \"selector\": \".charge-content\", \"type\": \"text\", }, { \"name\": \"course_name\", \"selector\": \".text-block-93\", \"type\": \"text\", }, { \"name\": \"course_description\", \"selector\": \".course-content-text\", \"type\": \"text\", }, { \"name\": \"course_icon\", \"selector\": \".image-92\", \"type\": \"attribute\", \"attribute\": \"src\", }, ], } browser_config = BrowserConfig(headless=True, java_script_enabled=True) js_click_tabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), js_code=[js_click_tabs], ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config ) companies = json.loads(result.extracted_content) print(f\"Successfully extracted {len(companies)} companies\") print(json.dumps(companies[0], indent=2)) async def main(): await extract_structured_data_using_css_extractor() if __name__ == \"__main__\": asyncio.run(main()) Key Points: BrowserConfig(headless=False): We want to watch it click “Next Page.” CrawlerRunConfig(...): We specify the extraction strategy, pass session_id to reuse the same page. js_code and wait_for are used for subsequent pages (page > 0) to click the “Next” button and wait for new commits to load. js_only=True indicates we’re not re-navigating but continuing the existing session. Finally, we call kill_session() to clean up the page and browser session. 9. Next Steps Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If you’re ready for more, check out: Installation: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks & Auth: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management: Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling! import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com\") print(result.markdown[:300]) # Print first 300 chars if __name__ == \"__main__\": asyncio.run(main()) import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browser_conf = BrowserConfig(headless=True) # or False to see the browser run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS ) async with AsyncWebCrawler(config=browser_conf) as crawler: result = await crawler.arun( url=\"https://example.com\", config=run_conf ) print(result.markdown) if __name__ == \"__main__\": asyncio.run(main()) from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_filter_strategy import PruningContentFilter from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator md_generator = DefaultMarkdownGenerator( content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\") ) config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, markdown_generator=md_generator ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://news.ycombinator.com\", config=config) print(\"Raw Markdown length:\", len(result.markdown.raw_markdown)) print(\"Fit Markdown length:\", len(result.markdown.fit_markdown)) from crawl4ai.extraction_strategy import JsonCssExtractionStrategy # Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" # Using OpenAI (requires API token) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"openai/gpt-4o\", # Default provider api_token=\"your-openai-token\" # Required for OpenAI ) # Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy.generate_schema( html, llm_provider=\"ollama/llama3.3\", # Open source alternative api_token=None # Not needed for Ollama ) # Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def main(): schema = { \"name\": \"Example Items\", \"baseSelector\": \"div.item\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"} ] } raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"raw://\" + raw_html, config=CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema) ) ) # The JSON output is stored in 'extracted_content' data = json.loads(result.extracted_content) print(data) if __name__ == \"__main__\": asyncio.run(main()) import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): model_name: str = Field(..., description=\"Name of the OpenAI model.\") input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") output_fee: str = Field( ..., description=\"Fee for output token for the OpenAI model.\" ) async def extract_structured_data_using_llm( provider: str, api_token: str = None, extra_headers: Dict[str, str] = None ): print(f\"\\n--- Extracting Structured Data with {provider} ---\") if api_token is None and provider != \"ollama\": print(f\"API token is required for {provider}. Skipping this example.\") return browser_config = BrowserConfig(headless=True) extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000} if extra_headers: extra_args[\"extra_headers\"] = extra_headers crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, word_count_threshold=1, page_timeout=80000, extraction_strategy=LLMExtractionStrategy( provider=provider, api_token=api_token, schema=OpenAIModelFee.model_json_schema(), extraction_type=\"schema\", instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\", extra_args=extra_args, ), ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://openai.com/api/pricing/\", config=crawler_config ) print(result.extracted_content) if __name__ == \"__main__\": # Use ollama with llama3.3 # asyncio.run( # extract_structured_data_using_llm( # provider=\"ollama/llama3.3\", api_token=\"no-token\" # ) # ) asyncio.run( extract_structured_data_using_llm( provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\") ) ) import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quick_parallel_example(): urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] run_conf = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, stream=True # Enable streaming mode ) async with AsyncWebCrawler() as crawler: # Stream results as they complete async for result in await crawler.arun_many(urls, config=run_conf): if result.success: print(f\"[OK] {result.url}, length: {len(result.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {result.url} => {result.error_message}\") # Or get all results at once (default behavior) run_conf = run_conf.clone(stream=False) results = await crawler.arun_many(urls, config=run_conf) for res in results: if res.success: print(f\"[OK] {res.url}, length: {len(res.markdown_v2.raw_markdown)}\") else: print(f\"[ERROR] {res.url} => {res.error_message}\") if __name__ == \"__main__\": asyncio.run(quick_parallel_example()) import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def extract_structured_data_using_css_extractor(): print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\") schema = { \"name\": \"KidoCode Courses\", \"baseSelector\": \"section.charge-methodology .w-tab-content > div\", \"fields\": [ { \"name\": \"section_title\", \"selector\": \"h3.heading-50\", \"type\": \"text\", }, { \"name\": \"section_description\", \"selector\": \".charge-content\", \"type\": \"text\", }, { \"name\": \"course_name\", \"selector\": \".text-block-93\", \"type\": \"text\", }, { \"name\": \"course_description\", \"selector\": \".course-content-text\", \"type\": \"text\", }, { \"name\": \"course_icon\", \"selector\": \".image-92\", \"type\": \"attribute\", \"attribute\": \"src\", }, ], } browser_config = BrowserConfig(headless=True, java_script_enabled=True) js_click_tabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawler_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), js_code=[js_click_tabs], ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun( url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config ) companies = json.loads(result.extracted_content) print(f\"Successfully extracted {len(companies)} companies\") print(json.dumps(companies[0], indent=2)) async def main(): await extract_structured_data_using_css_extractor() if __name__ == \"__main__\": asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-1": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-2": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-3": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-4": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-5": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-6": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-7": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-8": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-9": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-10": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-11": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-12": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-13": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/#__codelineno-0-14": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI 🚀🤖 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session re-use—fine-grained control. 4. High Performance: Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source: No forced API keys, no paywalls—everyone can access their data. Core Philosophies: - Democratize Data: Free to use, transparent, and highly configurable. - LLM Friendly: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we’ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you’ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether it’s a small fix, a big feature, or better docs—contributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyone—students, researchers, entrepreneurs, data scientists—to access, parse, and shape the world’s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let’s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! — Unclecode, Founder & Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main(): # Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: # Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") # Print the extracted content print(result.markdown) # Run the async main function asyncio.run(main()) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/core/installation/": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Installation & Setup (2023 Edition) 1. Basic Installation pip install crawl4ai This installs the core Crawl4AI library along with essential dependencies. No advanced features (like transformers or PyTorch) are included yet. 2. Initial Setup & Diagnostics 2.1 Run the Setup Command After installing, call: crawl4ai-setup What does it do? - Installs or updates required Playwright browsers (Chromium, Firefox, etc.) - Performs OS-level checks (e.g., missing libs on Linux) - Confirms your environment is ready to crawl 2.2 Diagnostics Optionally, you can run diagnostics to confirm everything is functioning: crawl4ai-doctor This command attempts to: - Check Python version compatibility - Verify Playwright installation - Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and re-run crawl4ai-setup. 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor) Below is a minimal Python script demonstrating a basic crawl. It uses our new BrowserConfig and CrawlerRunConfig for clarity, though no custom settings are passed in this example: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://www.example.com\", ) print(result.markdown[:300]) # Show the first 300 characters of extracted text if __name__ == \"__main__\": asyncio.run(main()) Expected outcome: - A headless browser session loads example.com - Crawl4AI returns ~300 characters of markdown. If errors occur, rerun crawl4ai-doctor or manually ensure Playwright is installed correctly. 4. Advanced Installation (Optional) Warning: Only install these if you truly need them. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly. 4.1 Torch, Transformers, or All Text Clustering (Torch) pip install crawl4ai[torch] crawl4ai-setup Installs PyTorch-based features (e.g., cosine similarity or advanced semantic chunking). Transformers pip install crawl4ai[transformer] crawl4ai-setup Adds Hugging Face-based summarization or generation strategies. All Features pip install crawl4ai[all] crawl4ai-setup (Optional) Pre-Fetching Models crawl4ai-download-models This step caches large models locally (if needed). Only do this if your workflow requires them. 5. Docker (Experimental) We provide a temporary Docker approach for testing. It’s not stable and may break with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try: docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic You can then make POST requests to http://localhost:11235/crawl to perform crawls. Production usage is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025). 6. Local Server Mode (Legacy) Some older docs mention running Crawl4AI as a local server. This approach has been partially replaced by the new Docker-based prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized. Summary 1. Install with pip install crawl4ai and run crawl4ai-setup. 2. Diagnose with crawl4ai-doctor if you see errors. 3. Verify by crawling example.com with minimal BrowserConfig + CrawlerRunConfig. 4. Advanced features (Torch, Transformers) are optional—avoid them if you don’t need them (they significantly increase resource usage). 5. Docker is experimental—use at your own risk until the stable version is released. 6. Local server references in older docs are largely deprecated; a new solution is in progress. Got questions? Check GitHub issues for updates or ask the community! Installation & Setup (2023 Edition) 1. Basic Installation pip install crawl4ai This installs the core Crawl4AI library along with essential dependencies. No advanced features (like transformers or PyTorch) are included yet. 2. Initial Setup & Diagnostics 2.1 Run the Setup Command After installing, call: crawl4ai-setup What does it do? - Installs or updates required Playwright browsers (Chromium, Firefox, etc.) - Performs OS-level checks (e.g., missing libs on Linux) - Confirms your environment is ready to crawl 2.2 Diagnostics Optionally, you can run diagnostics to confirm everything is functioning: crawl4ai-doctor This command attempts to: - Check Python version compatibility - Verify Playwright installation - Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and re-run crawl4ai-setup. 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor) Below is a minimal Python script demonstrating a basic crawl. It uses our new BrowserConfig and CrawlerRunConfig for clarity, though no custom settings are passed in this example: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://www.example.com\", ) print(result.markdown[:300]) # Show the first 300 characters of extracted text if __name__ == \"__main__\": asyncio.run(main()) Expected outcome: - A headless browser session loads example.com - Crawl4AI returns ~300 characters of markdown. If errors occur, rerun crawl4ai-doctor or manually ensure Playwright is installed correctly. 4. Advanced Installation (Optional) Warning: Only install these if you truly need them. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly. 4.1 Torch, Transformers, or All Text Clustering (Torch) pip install crawl4ai[torch] crawl4ai-setup Installs PyTorch-based features (e.g., cosine similarity or advanced semantic chunking). Transformers pip install crawl4ai[transformer] crawl4ai-setup Adds Hugging Face-based summarization or generation strategies. All Features pip install crawl4ai[all] crawl4ai-setup (Optional) Pre-Fetching Models crawl4ai-download-models This step caches large models locally (if needed). Only do this if your workflow requires them. 5. Docker (Experimental) We provide a temporary Docker approach for testing. It’s not stable and may break with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try: docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic You can then make POST requests to http://localhost:11235/crawl to perform crawls. Production usage is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025). 6. Local Server Mode (Legacy) Some older docs mention running Crawl4AI as a local server. This approach has been partially replaced by the new Docker-based prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized. Summary 1. Install with pip install crawl4ai and run crawl4ai-setup. 2. Diagnose with crawl4ai-doctor if you see errors. 3. Verify by crawling example.com with minimal BrowserConfig + CrawlerRunConfig. 4. Advanced features (Torch, Transformers) are optional—avoid them if you don’t need them (they significantly increase resource usage). 5. Docker is experimental—use at your own risk until the stable version is released. 6. Local server references in older docs are largely deprecated; a new solution is in progress. Got questions? Check GitHub issues for updates or ask the community! Installation & Setup (2023 Edition) 1. Basic Installation pip install crawl4ai This installs the core Crawl4AI library along with essential dependencies. No advanced features (like transformers or PyTorch) are included yet. 2. Initial Setup & Diagnostics 2.1 Run the Setup Command After installing, call: crawl4ai-setup What does it do? - Installs or updates required Playwright browsers (Chromium, Firefox, etc.) - Performs OS-level checks (e.g., missing libs on Linux) - Confirms your environment is ready to crawl 2.2 Diagnostics Optionally, you can run diagnostics to confirm everything is functioning: crawl4ai-doctor This command attempts to: - Check Python version compatibility - Verify Playwright installation - Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and re-run crawl4ai-setup. 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor) Below is a minimal Python script demonstrating a basic crawl. It uses our new BrowserConfig and CrawlerRunConfig for clarity, though no custom settings are passed in this example: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://www.example.com\", ) print(result.markdown[:300]) # Show the first 300 characters of extracted text if __name__ == \"__main__\": asyncio.run(main()) Expected outcome: - A headless browser session loads example.com - Crawl4AI returns ~300 characters of markdown. If errors occur, rerun crawl4ai-doctor or manually ensure Playwright is installed correctly. 4. Advanced Installation (Optional) Warning: Only install these if you truly need them. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly. 4.1 Torch, Transformers, or All Text Clustering (Torch) pip install crawl4ai[torch] crawl4ai-setup Installs PyTorch-based features (e.g., cosine similarity or advanced semantic chunking). Transformers pip install crawl4ai[transformer] crawl4ai-setup Adds Hugging Face-based summarization or generation strategies. All Features pip install crawl4ai[all] crawl4ai-setup (Optional) Pre-Fetching Models crawl4ai-download-models This step caches large models locally (if needed). Only do this if your workflow requires them. 5. Docker (Experimental) We provide a temporary Docker approach for testing. It’s not stable and may break with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try: docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic You can then make POST requests to http://localhost:11235/crawl to perform crawls. Production usage is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025). 6. Local Server Mode (Legacy) Some older docs mention running Crawl4AI as a local server. This approach has been partially replaced by the new Docker-based prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized. Summary 1. Install with pip install crawl4ai and run crawl4ai-setup. 2. Diagnose with crawl4ai-doctor if you see errors. 3. Verify by crawling example.com with minimal BrowserConfig + CrawlerRunConfig. 4. Advanced features (Torch, Transformers) are optional—avoid them if you don’t need them (they significantly increase resource usage). 5. Docker is experimental—use at your own risk until the stable version is released. 6. Local server references in older docs are largely deprecated; a new solution is in progress. Got questions? Check GitHub issues for updates or ask the community! Installation & Setup (2023 Edition) 1. Basic Installation pip install crawl4ai This installs the core Crawl4AI library along with essential dependencies. No advanced features (like transformers or PyTorch) are included yet. 2. Initial Setup & Diagnostics 2.1 Run the Setup Command After installing, call: crawl4ai-setup What does it do? - Installs or updates required Playwright browsers (Chromium, Firefox, etc.) - Performs OS-level checks (e.g., missing libs on Linux) - Confirms your environment is ready to crawl 2.2 Diagnostics Optionally, you can run diagnostics to confirm everything is functioning: crawl4ai-doctor This command attempts to: - Check Python version compatibility - Verify Playwright installation - Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and re-run crawl4ai-setup. 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor) Below is a minimal Python script demonstrating a basic crawl. It uses our new BrowserConfig and CrawlerRunConfig for clarity, though no custom settings are passed in this example: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://www.example.com\", ) print(result.markdown[:300]) # Show the first 300 characters of extracted text if __name__ == \"__main__\": asyncio.run(main()) Expected outcome: - A headless browser session loads example.com - Crawl4AI returns ~300 characters of markdown. If errors occur, rerun crawl4ai-doctor or manually ensure Playwright is installed correctly. 4. Advanced Installation (Optional) Warning: Only install these if you truly need them. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly. 4.1 Torch, Transformers, or All Text Clustering (Torch) pip install crawl4ai[torch] crawl4ai-setup Installs PyTorch-based features (e.g., cosine similarity or advanced semantic chunking). Transformers pip install crawl4ai[transformer] crawl4ai-setup Adds Hugging Face-based summarization or generation strategies. All Features pip install crawl4ai[all] crawl4ai-setup (Optional) Pre-Fetching Models crawl4ai-download-models This step caches large models locally (if needed). Only do this if your workflow requires them. 5. Docker (Experimental) We provide a temporary Docker approach for testing. It’s not stable and may break with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try: docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic You can then make POST requests to http://localhost:11235/crawl to perform crawls. Production usage is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025). 6. Local Server Mode (Legacy) Some older docs mention running Crawl4AI as a local server. This approach has been partially replaced by the new Docker-based prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized. Summary 1. Install with pip install crawl4ai and run crawl4ai-setup. 2. Diagnose with crawl4ai-doctor if you see errors. 3. Verify by crawling example.com with minimal BrowserConfig + CrawlerRunConfig. 4. Advanced features (Torch, Transformers) are optional—avoid them if you don’t need them (they significantly increase resource usage). 5. Docker is experimental—use at your own risk until the stable version is released. 6. Local server references in older docs are largely deprecated; a new solution is in progress. Got questions? Check GitHub issues for updates or ask the community! pip install crawl4ai crawl4ai-setup crawl4ai-doctor import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://www.example.com\", ) print(result.markdown[:300]) # Show the first 300 characters of extracted text if __name__ == \"__main__\": asyncio.run(main()) pip install crawl4ai[torch] crawl4ai-setup pip install crawl4ai[transformer] crawl4ai-setup pip install crawl4ai[all] crawl4ai-setup crawl4ai-download-models docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching",
  "https://crawl4ai.com/mkdocs/api/async-webcrawler/": "Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) Crawl4AI Documentation (v0.4.3bx) AsyncWebCrawler The AsyncWebCrawler is the core class for asynchronous web crawling in Crawl4AI. You typically create it once, optionally customize it with a BrowserConfig (e.g., headless, user agent), then run multiple arun() calls with different CrawlerRunConfig objects. Recommended usage: 1. Create a BrowserConfig for global browser settings. 2. Instantiate AsyncWebCrawler(config=browser_config). 3. Use the crawler in an async context manager (async with) or manage start/close manually. 4. Call arun(url, config=crawler_run_config) for each page you want. 1. Constructor Overview class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, config: Optional[BrowserConfig] = None, always_bypass_cache: bool = False, # deprecated always_by_pass_cache: Optional[bool] = None, # also deprecated base_directory: str = ..., thread_safe: bool = False, **kwargs, ): \"\"\" Create an AsyncWebCrawler instance. Args: crawler_strategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. always_bypass_cache: (Deprecated) Use CrawlerRunConfig.cache_mode instead. base_directory: Folder for storing caches/logs (if relevant). thread_safe: If True, attempts some concurrency safeguards. Usually False. **kwargs: Additional legacy or debugging parameters. \"\"\" ) ### Typical Initialization ```python from crawl4ai import AsyncWebCrawler, BrowserConfig browser_cfg = BrowserConfig( browser_type=\"chromium\", headless=True, verbose=True ) crawler = AsyncWebCrawler(config=browser_cfg) Notes: Legacy parameters like always_bypass_cache remain for backward compatibility, but prefer to set caching in CrawlerRunConfig. 2. Lifecycle: Start/Close or Context Manager 2.1 Context Manager (Recommended) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com\") # The crawler automatically starts/closes resources When the async with block ends, the crawler cleans up (closes the browser, etc.). 2.2 Manual Start & Close crawler = AsyncWebCrawler(config=browser_cfg) await crawler.start() result1 = await crawler.arun(\"https://example.com\") result2 = await crawler.arun(\"https://another.com\") await crawler.close() Use this style if you have a long-running application or need full control of the crawler’s lifecycle. 3. Primary Method: arun() async def arun( self, url: str, config: Optional[CrawlerRunConfig] = None, # Legacy parameters for backward compatibility... ) -> CrawlResult: ... 3.1 New Approach You pass a CrawlerRunConfig object that sets up everything about a crawl—content filtering, caching, session reuse, JS code, screenshots, etc. import asyncio from crawl4ai import CrawlerRunConfig, CacheMode run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, css_selector=\"main.article\", word_count_threshold=10, screenshot=True ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com/news\", config=run_cfg) print(\"Crawled HTML length:\", len(result.cleaned_html)) if result.screenshot: print(\"Screenshot base64 length:\", len(result.screenshot)) 3.2 Legacy Parameters Still Accepted For backward compatibility, arun() can still accept direct arguments like css_selector=..., word_count_threshold=..., etc., but we strongly advise migrating them into a CrawlerRunConfig. 4. Batch Processing: arun_many() async def arun_many( self, urls: List[str], config: Optional[CrawlerRunConfig] = None, # Legacy parameters maintained for backwards compatibility... ) -> List[CrawlResult]: \"\"\" Process multiple URLs with intelligent rate limiting and resource monitoring. \"\"\" 4.1 Resource-Aware Crawling The arun_many() method now uses an intelligent dispatcher that: Monitors system memory usage Implements adaptive rate limiting Provides detailed progress monitoring Manages concurrent crawls efficiently 4.2 Example Usage from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode # Configure browser browser_cfg = BrowserConfig(headless=True) # Configure crawler with rate limiting run_cfg = CrawlerRunConfig( # Enable rate limiting enable_rate_limiting=True, rate_limit_config=RateLimitConfig( base_delay=(1.0, 2.0), # Random delay between 1-2 seconds max_delay=30.0, # Maximum delay after rate limit hits max_retries=2, # Number of retries before giving up rate_limit_codes=[429, 503] # Status codes that trigger rate limiting ), # Resource monitoring memory_threshold_percent=70.0, # Pause if memory exceeds this check_interval=0.5, # How often to check resources max_session_permit=3, # Maximum concurrent crawls display_mode=DisplayMode.DETAILED.value # Show detailed progress ) urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] async with AsyncWebCrawler(config=browser_cfg) as crawler: results = await crawler.arun_many(urls, config=run_cfg) for result in results: print(f\"URL: {result.url}, Success: {result.success}\") 4.3 Key Features 1. Rate Limiting Automatic delay between requests Exponential backoff on rate limit detection Domain-specific rate limiting Configurable retry strategy 2. Resource Monitoring Memory usage tracking Adaptive concurrency based on system load Automatic pausing when resources are constrained 3. Progress Monitoring Detailed or aggregated progress display Real-time status updates Memory usage statistics 4. Error Handling Graceful handling of rate limits Automatic retries with backoff Detailed error reporting 5. CrawlResult Output Each arun() returns a CrawlResult containing: url: Final URL (if redirected). html: Original HTML. cleaned_html: Sanitized HTML. markdown_v2 (or future markdown): Markdown outputs (raw, fit, etc.). extracted_content: If an extraction strategy was used (JSON for CSS/LLM strategies). screenshot, pdf: If screenshots/PDF requested. media, links: Information about discovered images/links. success, error_message: Status info. For details, see CrawlResult doc. 6. Quick Example Below is an example hooking it all together: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy import json async def main(): # 1. Browser config browser_cfg = BrowserConfig( browser_type=\"firefox\", headless=False, verbose=True ) # 2. Run config schema = { \"name\": \"Articles\", \"baseSelector\": \"article.post\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\" }, { \"name\": \"url\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\" } ] } run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), word_count_threshold=15, remove_overlay_elements=True, wait_for=\"css:.post\" # Wait for posts to appear ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun( url=\"https://example.com/blog\", config=run_cfg ) if result.success: print(\"Cleaned HTML length:\", len(result.cleaned_html)) if result.extracted_content: articles = json.loads(result.extracted_content) print(\"Extracted articles:\", articles[:2]) else: print(\"Error:\", result.error_message) asyncio.run(main()) Explanation: We define a BrowserConfig with Firefox, no headless, and verbose=True. We define a CrawlerRunConfig that bypasses cache, uses a CSS extraction schema, has a word_count_threshold=15, etc. We pass them to AsyncWebCrawler(config=...) and arun(url=..., config=...). 7. Best Practices & Migration Notes 1. Use BrowserConfig for global settings about the browser’s environment. 2. Use CrawlerRunConfig for per-crawl logic (caching, content filtering, extraction strategies, wait conditions). 3. Avoid legacy parameters like css_selector or word_count_threshold directly in arun(). Instead: run_cfg = CrawlerRunConfig(css_selector=\".main-content\", word_count_threshold=20) result = await crawler.arun(url=\"...\", config=run_cfg) 4. Context Manager usage is simplest unless you want a persistent crawler across many calls. 8. Summary AsyncWebCrawler is your entry point to asynchronous crawling: Constructor accepts BrowserConfig (or defaults). arun(url, config=CrawlerRunConfig) is the main method for single-page crawls. arun_many(urls, config=CrawlerRunConfig) handles concurrency across multiple URLs. For advanced lifecycle control, use start() and close() explicitly. Migration: If you used AsyncWebCrawler(browser_type=\"chromium\", css_selector=\"...\"), move browser settings to BrowserConfig(...) and content/crawl logic to CrawlerRunConfig(...). This modular approach ensures your code is clean, scalable, and easy to maintain. For any advanced or rarely used parameters, see the BrowserConfig docs. AsyncWebCrawler The AsyncWebCrawler is the core class for asynchronous web crawling in Crawl4AI. You typically create it once, optionally customize it with a BrowserConfig (e.g., headless, user agent), then run multiple arun() calls with different CrawlerRunConfig objects. Recommended usage: 1. Create a BrowserConfig for global browser settings. 2. Instantiate AsyncWebCrawler(config=browser_config). 3. Use the crawler in an async context manager (async with) or manage start/close manually. 4. Call arun(url, config=crawler_run_config) for each page you want. 1. Constructor Overview class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, config: Optional[BrowserConfig] = None, always_bypass_cache: bool = False, # deprecated always_by_pass_cache: Optional[bool] = None, # also deprecated base_directory: str = ..., thread_safe: bool = False, **kwargs, ): \"\"\" Create an AsyncWebCrawler instance. Args: crawler_strategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. always_bypass_cache: (Deprecated) Use CrawlerRunConfig.cache_mode instead. base_directory: Folder for storing caches/logs (if relevant). thread_safe: If True, attempts some concurrency safeguards. Usually False. **kwargs: Additional legacy or debugging parameters. \"\"\" ) ### Typical Initialization ```python from crawl4ai import AsyncWebCrawler, BrowserConfig browser_cfg = BrowserConfig( browser_type=\"chromium\", headless=True, verbose=True ) crawler = AsyncWebCrawler(config=browser_cfg) Notes: Legacy parameters like always_bypass_cache remain for backward compatibility, but prefer to set caching in CrawlerRunConfig. 2. Lifecycle: Start/Close or Context Manager 2.1 Context Manager (Recommended) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com\") # The crawler automatically starts/closes resources When the async with block ends, the crawler cleans up (closes the browser, etc.). 2.2 Manual Start & Close crawler = AsyncWebCrawler(config=browser_cfg) await crawler.start() result1 = await crawler.arun(\"https://example.com\") result2 = await crawler.arun(\"https://another.com\") await crawler.close() Use this style if you have a long-running application or need full control of the crawler’s lifecycle. 3. Primary Method: arun() async def arun( self, url: str, config: Optional[CrawlerRunConfig] = None, # Legacy parameters for backward compatibility... ) -> CrawlResult: ... 3.1 New Approach You pass a CrawlerRunConfig object that sets up everything about a crawl—content filtering, caching, session reuse, JS code, screenshots, etc. import asyncio from crawl4ai import CrawlerRunConfig, CacheMode run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, css_selector=\"main.article\", word_count_threshold=10, screenshot=True ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com/news\", config=run_cfg) print(\"Crawled HTML length:\", len(result.cleaned_html)) if result.screenshot: print(\"Screenshot base64 length:\", len(result.screenshot)) 3.2 Legacy Parameters Still Accepted For backward compatibility, arun() can still accept direct arguments like css_selector=..., word_count_threshold=..., etc., but we strongly advise migrating them into a CrawlerRunConfig. 4. Batch Processing: arun_many() async def arun_many( self, urls: List[str], config: Optional[CrawlerRunConfig] = None, # Legacy parameters maintained for backwards compatibility... ) -> List[CrawlResult]: \"\"\" Process multiple URLs with intelligent rate limiting and resource monitoring. \"\"\" 4.1 Resource-Aware Crawling The arun_many() method now uses an intelligent dispatcher that: Monitors system memory usage Implements adaptive rate limiting Provides detailed progress monitoring Manages concurrent crawls efficiently 4.2 Example Usage from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode # Configure browser browser_cfg = BrowserConfig(headless=True) # Configure crawler with rate limiting run_cfg = CrawlerRunConfig( # Enable rate limiting enable_rate_limiting=True, rate_limit_config=RateLimitConfig( base_delay=(1.0, 2.0), # Random delay between 1-2 seconds max_delay=30.0, # Maximum delay after rate limit hits max_retries=2, # Number of retries before giving up rate_limit_codes=[429, 503] # Status codes that trigger rate limiting ), # Resource monitoring memory_threshold_percent=70.0, # Pause if memory exceeds this check_interval=0.5, # How often to check resources max_session_permit=3, # Maximum concurrent crawls display_mode=DisplayMode.DETAILED.value # Show detailed progress ) urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] async with AsyncWebCrawler(config=browser_cfg) as crawler: results = await crawler.arun_many(urls, config=run_cfg) for result in results: print(f\"URL: {result.url}, Success: {result.success}\") 4.3 Key Features 1. Rate Limiting Automatic delay between requests Exponential backoff on rate limit detection Domain-specific rate limiting Configurable retry strategy 2. Resource Monitoring Memory usage tracking Adaptive concurrency based on system load Automatic pausing when resources are constrained 3. Progress Monitoring Detailed or aggregated progress display Real-time status updates Memory usage statistics 4. Error Handling Graceful handling of rate limits Automatic retries with backoff Detailed error reporting 5. CrawlResult Output Each arun() returns a CrawlResult containing: url: Final URL (if redirected). html: Original HTML. cleaned_html: Sanitized HTML. markdown_v2 (or future markdown): Markdown outputs (raw, fit, etc.). extracted_content: If an extraction strategy was used (JSON for CSS/LLM strategies). screenshot, pdf: If screenshots/PDF requested. media, links: Information about discovered images/links. success, error_message: Status info. For details, see CrawlResult doc. 6. Quick Example Below is an example hooking it all together: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy import json async def main(): # 1. Browser config browser_cfg = BrowserConfig( browser_type=\"firefox\", headless=False, verbose=True ) # 2. Run config schema = { \"name\": \"Articles\", \"baseSelector\": \"article.post\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\" }, { \"name\": \"url\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\" } ] } run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), word_count_threshold=15, remove_overlay_elements=True, wait_for=\"css:.post\" # Wait for posts to appear ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun( url=\"https://example.com/blog\", config=run_cfg ) if result.success: print(\"Cleaned HTML length:\", len(result.cleaned_html)) if result.extracted_content: articles = json.loads(result.extracted_content) print(\"Extracted articles:\", articles[:2]) else: print(\"Error:\", result.error_message) asyncio.run(main()) Explanation: We define a BrowserConfig with Firefox, no headless, and verbose=True. We define a CrawlerRunConfig that bypasses cache, uses a CSS extraction schema, has a word_count_threshold=15, etc. We pass them to AsyncWebCrawler(config=...) and arun(url=..., config=...). 7. Best Practices & Migration Notes 1. Use BrowserConfig for global settings about the browser’s environment. 2. Use CrawlerRunConfig for per-crawl logic (caching, content filtering, extraction strategies, wait conditions). 3. Avoid legacy parameters like css_selector or word_count_threshold directly in arun(). Instead: run_cfg = CrawlerRunConfig(css_selector=\".main-content\", word_count_threshold=20) result = await crawler.arun(url=\"...\", config=run_cfg) 4. Context Manager usage is simplest unless you want a persistent crawler across many calls. 8. Summary AsyncWebCrawler is your entry point to asynchronous crawling: Constructor accepts BrowserConfig (or defaults). arun(url, config=CrawlerRunConfig) is the main method for single-page crawls. arun_many(urls, config=CrawlerRunConfig) handles concurrency across multiple URLs. For advanced lifecycle control, use start() and close() explicitly. Migration: If you used AsyncWebCrawler(browser_type=\"chromium\", css_selector=\"...\"), move browser settings to BrowserConfig(...) and content/crawl logic to CrawlerRunConfig(...). This modular approach ensures your code is clean, scalable, and easy to maintain. For any advanced or rarely used parameters, see the BrowserConfig docs. AsyncWebCrawler The AsyncWebCrawler is the core class for asynchronous web crawling in Crawl4AI. You typically create it once, optionally customize it with a BrowserConfig (e.g., headless, user agent), then run multiple arun() calls with different CrawlerRunConfig objects. Recommended usage: 1. Create a BrowserConfig for global browser settings. 2. Instantiate AsyncWebCrawler(config=browser_config). 3. Use the crawler in an async context manager (async with) or manage start/close manually. 4. Call arun(url, config=crawler_run_config) for each page you want. 1. Constructor Overview class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, config: Optional[BrowserConfig] = None, always_bypass_cache: bool = False, # deprecated always_by_pass_cache: Optional[bool] = None, # also deprecated base_directory: str = ..., thread_safe: bool = False, **kwargs, ): \"\"\" Create an AsyncWebCrawler instance. Args: crawler_strategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. always_bypass_cache: (Deprecated) Use CrawlerRunConfig.cache_mode instead. base_directory: Folder for storing caches/logs (if relevant). thread_safe: If True, attempts some concurrency safeguards. Usually False. **kwargs: Additional legacy or debugging parameters. \"\"\" ) ### Typical Initialization ```python from crawl4ai import AsyncWebCrawler, BrowserConfig browser_cfg = BrowserConfig( browser_type=\"chromium\", headless=True, verbose=True ) crawler = AsyncWebCrawler(config=browser_cfg) Notes: Legacy parameters like always_bypass_cache remain for backward compatibility, but prefer to set caching in CrawlerRunConfig. 2. Lifecycle: Start/Close or Context Manager 2.1 Context Manager (Recommended) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com\") # The crawler automatically starts/closes resources When the async with block ends, the crawler cleans up (closes the browser, etc.). 2.2 Manual Start & Close crawler = AsyncWebCrawler(config=browser_cfg) await crawler.start() result1 = await crawler.arun(\"https://example.com\") result2 = await crawler.arun(\"https://another.com\") await crawler.close() Use this style if you have a long-running application or need full control of the crawler’s lifecycle. 3. Primary Method: arun() async def arun( self, url: str, config: Optional[CrawlerRunConfig] = None, # Legacy parameters for backward compatibility... ) -> CrawlResult: ... 3.1 New Approach You pass a CrawlerRunConfig object that sets up everything about a crawl—content filtering, caching, session reuse, JS code, screenshots, etc. import asyncio from crawl4ai import CrawlerRunConfig, CacheMode run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, css_selector=\"main.article\", word_count_threshold=10, screenshot=True ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com/news\", config=run_cfg) print(\"Crawled HTML length:\", len(result.cleaned_html)) if result.screenshot: print(\"Screenshot base64 length:\", len(result.screenshot)) 3.2 Legacy Parameters Still Accepted For backward compatibility, arun() can still accept direct arguments like css_selector=..., word_count_threshold=..., etc., but we strongly advise migrating them into a CrawlerRunConfig. 4. Batch Processing: arun_many() async def arun_many( self, urls: List[str], config: Optional[CrawlerRunConfig] = None, # Legacy parameters maintained for backwards compatibility... ) -> List[CrawlResult]: \"\"\" Process multiple URLs with intelligent rate limiting and resource monitoring. \"\"\" 4.1 Resource-Aware Crawling The arun_many() method now uses an intelligent dispatcher that: Monitors system memory usage Implements adaptive rate limiting Provides detailed progress monitoring Manages concurrent crawls efficiently 4.2 Example Usage from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode # Configure browser browser_cfg = BrowserConfig(headless=True) # Configure crawler with rate limiting run_cfg = CrawlerRunConfig( # Enable rate limiting enable_rate_limiting=True, rate_limit_config=RateLimitConfig( base_delay=(1.0, 2.0), # Random delay between 1-2 seconds max_delay=30.0, # Maximum delay after rate limit hits max_retries=2, # Number of retries before giving up rate_limit_codes=[429, 503] # Status codes that trigger rate limiting ), # Resource monitoring memory_threshold_percent=70.0, # Pause if memory exceeds this check_interval=0.5, # How often to check resources max_session_permit=3, # Maximum concurrent crawls display_mode=DisplayMode.DETAILED.value # Show detailed progress ) urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] async with AsyncWebCrawler(config=browser_cfg) as crawler: results = await crawler.arun_many(urls, config=run_cfg) for result in results: print(f\"URL: {result.url}, Success: {result.success}\") 4.3 Key Features 1. Rate Limiting Automatic delay between requests Exponential backoff on rate limit detection Domain-specific rate limiting Configurable retry strategy 2. Resource Monitoring Memory usage tracking Adaptive concurrency based on system load Automatic pausing when resources are constrained 3. Progress Monitoring Detailed or aggregated progress display Real-time status updates Memory usage statistics 4. Error Handling Graceful handling of rate limits Automatic retries with backoff Detailed error reporting 5. CrawlResult Output Each arun() returns a CrawlResult containing: url: Final URL (if redirected). html: Original HTML. cleaned_html: Sanitized HTML. markdown_v2 (or future markdown): Markdown outputs (raw, fit, etc.). extracted_content: If an extraction strategy was used (JSON for CSS/LLM strategies). screenshot, pdf: If screenshots/PDF requested. media, links: Information about discovered images/links. success, error_message: Status info. For details, see CrawlResult doc. 6. Quick Example Below is an example hooking it all together: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy import json async def main(): # 1. Browser config browser_cfg = BrowserConfig( browser_type=\"firefox\", headless=False, verbose=True ) # 2. Run config schema = { \"name\": \"Articles\", \"baseSelector\": \"article.post\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\" }, { \"name\": \"url\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\" } ] } run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), word_count_threshold=15, remove_overlay_elements=True, wait_for=\"css:.post\" # Wait for posts to appear ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun( url=\"https://example.com/blog\", config=run_cfg ) if result.success: print(\"Cleaned HTML length:\", len(result.cleaned_html)) if result.extracted_content: articles = json.loads(result.extracted_content) print(\"Extracted articles:\", articles[:2]) else: print(\"Error:\", result.error_message) asyncio.run(main()) Explanation: We define a BrowserConfig with Firefox, no headless, and verbose=True. We define a CrawlerRunConfig that bypasses cache, uses a CSS extraction schema, has a word_count_threshold=15, etc. We pass them to AsyncWebCrawler(config=...) and arun(url=..., config=...). 7. Best Practices & Migration Notes 1. Use BrowserConfig for global settings about the browser’s environment. 2. Use CrawlerRunConfig for per-crawl logic (caching, content filtering, extraction strategies, wait conditions). 3. Avoid legacy parameters like css_selector or word_count_threshold directly in arun(). Instead: run_cfg = CrawlerRunConfig(css_selector=\".main-content\", word_count_threshold=20) result = await crawler.arun(url=\"...\", config=run_cfg) 4. Context Manager usage is simplest unless you want a persistent crawler across many calls. 8. Summary AsyncWebCrawler is your entry point to asynchronous crawling: Constructor accepts BrowserConfig (or defaults). arun(url, config=CrawlerRunConfig) is the main method for single-page crawls. arun_many(urls, config=CrawlerRunConfig) handles concurrency across multiple URLs. For advanced lifecycle control, use start() and close() explicitly. Migration: If you used AsyncWebCrawler(browser_type=\"chromium\", css_selector=\"...\"), move browser settings to BrowserConfig(...) and content/crawl logic to CrawlerRunConfig(...). This modular approach ensures your code is clean, scalable, and easy to maintain. For any advanced or rarely used parameters, see the BrowserConfig docs. AsyncWebCrawler The AsyncWebCrawler is the core class for asynchronous web crawling in Crawl4AI. You typically create it once, optionally customize it with a BrowserConfig (e.g., headless, user agent), then run multiple arun() calls with different CrawlerRunConfig objects. Recommended usage: 1. Create a BrowserConfig for global browser settings. 2. Instantiate AsyncWebCrawler(config=browser_config). 3. Use the crawler in an async context manager (async with) or manage start/close manually. 4. Call arun(url, config=crawler_run_config) for each page you want. 1. Constructor Overview class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, config: Optional[BrowserConfig] = None, always_bypass_cache: bool = False, # deprecated always_by_pass_cache: Optional[bool] = None, # also deprecated base_directory: str = ..., thread_safe: bool = False, **kwargs, ): \"\"\" Create an AsyncWebCrawler instance. Args: crawler_strategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. always_bypass_cache: (Deprecated) Use CrawlerRunConfig.cache_mode instead. base_directory: Folder for storing caches/logs (if relevant). thread_safe: If True, attempts some concurrency safeguards. Usually False. **kwargs: Additional legacy or debugging parameters. \"\"\" ) ### Typical Initialization ```python from crawl4ai import AsyncWebCrawler, BrowserConfig browser_cfg = BrowserConfig( browser_type=\"chromium\", headless=True, verbose=True ) crawler = AsyncWebCrawler(config=browser_cfg) Notes: Legacy parameters like always_bypass_cache remain for backward compatibility, but prefer to set caching in CrawlerRunConfig. 2. Lifecycle: Start/Close or Context Manager 2.1 Context Manager (Recommended) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com\") # The crawler automatically starts/closes resources When the async with block ends, the crawler cleans up (closes the browser, etc.). 2.2 Manual Start & Close crawler = AsyncWebCrawler(config=browser_cfg) await crawler.start() result1 = await crawler.arun(\"https://example.com\") result2 = await crawler.arun(\"https://another.com\") await crawler.close() Use this style if you have a long-running application or need full control of the crawler’s lifecycle. 3. Primary Method: arun() async def arun( self, url: str, config: Optional[CrawlerRunConfig] = None, # Legacy parameters for backward compatibility... ) -> CrawlResult: ... 3.1 New Approach You pass a CrawlerRunConfig object that sets up everything about a crawl—content filtering, caching, session reuse, JS code, screenshots, etc. import asyncio from crawl4ai import CrawlerRunConfig, CacheMode run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, css_selector=\"main.article\", word_count_threshold=10, screenshot=True ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com/news\", config=run_cfg) print(\"Crawled HTML length:\", len(result.cleaned_html)) if result.screenshot: print(\"Screenshot base64 length:\", len(result.screenshot)) 3.2 Legacy Parameters Still Accepted For backward compatibility, arun() can still accept direct arguments like css_selector=..., word_count_threshold=..., etc., but we strongly advise migrating them into a CrawlerRunConfig. 4. Batch Processing: arun_many() async def arun_many( self, urls: List[str], config: Optional[CrawlerRunConfig] = None, # Legacy parameters maintained for backwards compatibility... ) -> List[CrawlResult]: \"\"\" Process multiple URLs with intelligent rate limiting and resource monitoring. \"\"\" 4.1 Resource-Aware Crawling The arun_many() method now uses an intelligent dispatcher that: Monitors system memory usage Implements adaptive rate limiting Provides detailed progress monitoring Manages concurrent crawls efficiently 4.2 Example Usage from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode # Configure browser browser_cfg = BrowserConfig(headless=True) # Configure crawler with rate limiting run_cfg = CrawlerRunConfig( # Enable rate limiting enable_rate_limiting=True, rate_limit_config=RateLimitConfig( base_delay=(1.0, 2.0), # Random delay between 1-2 seconds max_delay=30.0, # Maximum delay after rate limit hits max_retries=2, # Number of retries before giving up rate_limit_codes=[429, 503] # Status codes that trigger rate limiting ), # Resource monitoring memory_threshold_percent=70.0, # Pause if memory exceeds this check_interval=0.5, # How often to check resources max_session_permit=3, # Maximum concurrent crawls display_mode=DisplayMode.DETAILED.value # Show detailed progress ) urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] async with AsyncWebCrawler(config=browser_cfg) as crawler: results = await crawler.arun_many(urls, config=run_cfg) for result in results: print(f\"URL: {result.url}, Success: {result.success}\") 4.3 Key Features 1. Rate Limiting Automatic delay between requests Exponential backoff on rate limit detection Domain-specific rate limiting Configurable retry strategy 2. Resource Monitoring Memory usage tracking Adaptive concurrency based on system load Automatic pausing when resources are constrained 3. Progress Monitoring Detailed or aggregated progress display Real-time status updates Memory usage statistics 4. Error Handling Graceful handling of rate limits Automatic retries with backoff Detailed error reporting 5. CrawlResult Output Each arun() returns a CrawlResult containing: url: Final URL (if redirected). html: Original HTML. cleaned_html: Sanitized HTML. markdown_v2 (or future markdown): Markdown outputs (raw, fit, etc.). extracted_content: If an extraction strategy was used (JSON for CSS/LLM strategies). screenshot, pdf: If screenshots/PDF requested. media, links: Information about discovered images/links. success, error_message: Status info. For details, see CrawlResult doc. 6. Quick Example Below is an example hooking it all together: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy import json async def main(): # 1. Browser config browser_cfg = BrowserConfig( browser_type=\"firefox\", headless=False, verbose=True ) # 2. Run config schema = { \"name\": \"Articles\", \"baseSelector\": \"article.post\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\" }, { \"name\": \"url\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\" } ] } run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), word_count_threshold=15, remove_overlay_elements=True, wait_for=\"css:.post\" # Wait for posts to appear ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun( url=\"https://example.com/blog\", config=run_cfg ) if result.success: print(\"Cleaned HTML length:\", len(result.cleaned_html)) if result.extracted_content: articles = json.loads(result.extracted_content) print(\"Extracted articles:\", articles[:2]) else: print(\"Error:\", result.error_message) asyncio.run(main()) Explanation: We define a BrowserConfig with Firefox, no headless, and verbose=True. We define a CrawlerRunConfig that bypasses cache, uses a CSS extraction schema, has a word_count_threshold=15, etc. We pass them to AsyncWebCrawler(config=...) and arun(url=..., config=...). 7. Best Practices & Migration Notes 1. Use BrowserConfig for global settings about the browser’s environment. 2. Use CrawlerRunConfig for per-crawl logic (caching, content filtering, extraction strategies, wait conditions). 3. Avoid legacy parameters like css_selector or word_count_threshold directly in arun(). Instead: run_cfg = CrawlerRunConfig(css_selector=\".main-content\", word_count_threshold=20) result = await crawler.arun(url=\"...\", config=run_cfg) 4. Context Manager usage is simplest unless you want a persistent crawler across many calls. 8. Summary AsyncWebCrawler is your entry point to asynchronous crawling: Constructor accepts BrowserConfig (or defaults). arun(url, config=CrawlerRunConfig) is the main method for single-page crawls. arun_many(urls, config=CrawlerRunConfig) handles concurrency across multiple URLs. For advanced lifecycle control, use start() and close() explicitly. Migration: If you used AsyncWebCrawler(browser_type=\"chromium\", css_selector=\"...\"), move browser settings to BrowserConfig(...) and content/crawl logic to CrawlerRunConfig(...). This modular approach ensures your code is clean, scalable, and easy to maintain. For any advanced or rarely used parameters, see the BrowserConfig docs. class AsyncWebCrawler: def __init__( self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, config: Optional[BrowserConfig] = None, always_bypass_cache: bool = False, # deprecated always_by_pass_cache: Optional[bool] = None, # also deprecated base_directory: str = ..., thread_safe: bool = False, **kwargs, ): \"\"\" Create an AsyncWebCrawler instance. Args: crawler_strategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. always_bypass_cache: (Deprecated) Use CrawlerRunConfig.cache_mode instead. base_directory: Folder for storing caches/logs (if relevant). thread_safe: If True, attempts some concurrency safeguards. Usually False. **kwargs: Additional legacy or debugging parameters. \"\"\" ) ### Typical Initialization ```python from crawl4ai import AsyncWebCrawler, BrowserConfig browser_cfg = BrowserConfig( browser_type=\"chromium\", headless=True, verbose=True ) crawler = AsyncWebCrawler(config=browser_cfg) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com\") # The crawler automatically starts/closes resources crawler = AsyncWebCrawler(config=browser_cfg) await crawler.start() result1 = await crawler.arun(\"https://example.com\") result2 = await crawler.arun(\"https://another.com\") await crawler.close() async def arun( self, url: str, config: Optional[CrawlerRunConfig] = None, # Legacy parameters for backward compatibility... ) -> CrawlResult: ... import asyncio from crawl4ai import CrawlerRunConfig, CacheMode run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, css_selector=\"main.article\", word_count_threshold=10, screenshot=True ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun(\"https://example.com/news\", config=run_cfg) print(\"Crawled HTML length:\", len(result.cleaned_html)) if result.screenshot: print(\"Screenshot base64 length:\", len(result.screenshot)) async def arun_many( self, urls: List[str], config: Optional[CrawlerRunConfig] = None, # Legacy parameters maintained for backwards compatibility... ) -> List[CrawlResult]: \"\"\" Process multiple URLs with intelligent rate limiting and resource monitoring. \"\"\" from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode # Configure browser browser_cfg = BrowserConfig(headless=True) # Configure crawler with rate limiting run_cfg = CrawlerRunConfig( # Enable rate limiting enable_rate_limiting=True, rate_limit_config=RateLimitConfig( base_delay=(1.0, 2.0), # Random delay between 1-2 seconds max_delay=30.0, # Maximum delay after rate limit hits max_retries=2, # Number of retries before giving up rate_limit_codes=[429, 503] # Status codes that trigger rate limiting ), # Resource monitoring memory_threshold_percent=70.0, # Pause if memory exceeds this check_interval=0.5, # How often to check resources max_session_permit=3, # Maximum concurrent crawls display_mode=DisplayMode.DETAILED.value # Show detailed progress ) urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] async with AsyncWebCrawler(config=browser_cfg) as crawler: results = await crawler.arun_many(urls, config=run_cfg) for result in results: print(f\"URL: {result.url}, Success: {result.success}\") import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy import json async def main(): # 1. Browser config browser_cfg = BrowserConfig( browser_type=\"firefox\", headless=False, verbose=True ) # 2. Run config schema = { \"name\": \"Articles\", \"baseSelector\": \"article.post\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\" }, { \"name\": \"url\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\" } ] } run_cfg = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, extraction_strategy=JsonCssExtractionStrategy(schema), word_count_threshold=15, remove_overlay_elements=True, wait_for=\"css:.post\" # Wait for posts to appear ) async with AsyncWebCrawler(config=browser_cfg) as crawler: result = await crawler.arun( url=\"https://example.com/blog\", config=run_cfg ) if result.success: print(\"Cleaned HTML length:\", len(result.cleaned_html)) if result.extracted_content: articles = json.loads(result.extracted_content) print(\"Extracted articles:\", articles[:2]) else: print(\"Error:\", result.error_message) asyncio.run(main()) run_cfg = CrawlerRunConfig(css_selector=\".main-content\", word_count_threshold=20) result = await crawler.arun(url=\"...\", config=run_cfg) Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching Search xClose Type to start searching"
}