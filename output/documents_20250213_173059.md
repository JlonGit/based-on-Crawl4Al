# 文档知识库

生成时间：2025-02-13 17:30:59

## 目录

1. [https://crawl4ai.com/mkdocs/](#doc-1)
2. [https://crawl4ai.com/mkdocs/core/quickstart/](#doc-2)
3. [https://crawl4ai.com/mkdocs/core/installation/](#doc-3)
4. [https://crawl4ai.com/mkdocs/api/async-webcrawler/](#doc-4)

---

## 文档 1 <span id='doc-1'></span>

### 元数据

- **来源URL**: https://crawl4ai.com/mkdocs/
- **字数统计**: 14409
- **抓取时间**: 2025-02-13 17:30:59

### 内容

> Crawl4AI: OpenSource LLMFriendly Web Crawler  Scraper Crawl4AI is the 1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazingfast, AIready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for realtime performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If youre looking for the old documentation, you can access it here. Quick Start Heres a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main():  Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler:  Run the crawler on a URL result  await crawler.arun(urlhttps:crawl4ai.com)  Print the extracted content print(result.markdown)  Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a featurerich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLMbased extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session reusefinegrained control. 4. High Performance: Parallel crawling, chunkbased extraction, realtime use cases. 5. Open Source: No forced API keys, no paywallseveryone can access their data. Core Philosophies:  Democratize Data: Free to use, transparent, and highly configurable.  LLM Friendly: Minimally processed, wellstructured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, weve organized our docs into clear sections: Setup  Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A handson introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on singlepage crawling, advanced browsercrawler parameters, content filtering, and caching. Advanced Explore link  media handling, lazy loading, hooking  authentication, proxies, session management, and more. Extraction Detailed references for noLLM (CSS, XPath) vs. LLMbased strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, youll find code samples you can copypaste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star  Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether its a small fix, a big feature, or better docscontributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyonestudents, researchers, entrepreneurs, data scientiststo access, parse, and shape the worlds data with speed, costefficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Lets keep building an open, democratic approach to data extraction and AI together. Happy Crawling!  Unclecode, Founder  Maintainer of Crawl4AI  Crawl4AI: OpenSource LLMFriendly Web Crawler  Scraper Crawl4AI is the 1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazingfast, AIready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for realtime performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If youre looking for the old documentation, you can access it here. Quick Start Heres a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main():  Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler:  Run the crawler on a URL result  await crawler.arun(urlhttps:crawl4ai.com)  Print the extracted content print(result.markdown)  Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a featurerich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLMbased extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session reusefinegrained control. 4. High Performance: Parallel crawling, chunkbased extraction, realtime use cases. 5. Open Source: No forced API keys, no paywallseveryone can access their data. Core Philosophies:  Democratize Data: Free to use, transparent, and highly configurable.  LLM Friendly: Minimally processed, wellstructured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, weve organized our docs into clear sections: Setup  Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A handson introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on singlepage crawling, advanced browsercrawler parameters, content filtering, and caching. Advanced Explore link  media handling, lazy loading, hooking  authentication, proxies, session management, and more. Extraction Detailed references for noLLM (CSS, XPath) vs. LLMbased strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, youll find code samples you can copypaste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star  Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether its a small fix, a big feature, or better docscontributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyonestudents, researchers, entrepreneurs, data scientiststo access, parse, and shape the worlds data with speed, costefficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Lets keep building an open, democratic approach to data extraction and AI together. Happy Crawling!  Unclecode, Founder  Maintainer of Crawl4AI  Crawl4AI: OpenSource LLMFriendly Web Crawler  Scraper Crawl4AI is the 1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazingfast, AIready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for realtime performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If youre looking for the old documentation, you can access it here. Quick Start Heres a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main():  Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler:  Run the crawler on a URL result  await crawler.arun(urlhttps:crawl4ai.com)  Print the extracted content print(result.markdown)  Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a featurerich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLMbased extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session reusefinegrained control. 4. High Performance: Parallel crawling, chunkbased extraction, realtime use cases. 5. Open Source: No forced API keys, no paywallseveryone can access their data. Core Philosophies:  Democratize Data: Free to use, transparent, and highly configurable.  LLM Friendly: Minimally processed, wellstructured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, weve organized our docs into clear sections: Setup  Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A handson introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on singlepage crawling, advanced browsercrawler parameters, content filtering, and caching. Advanced Explore link  media handling, lazy loading, hooking  authentication, proxies, session management, and more. Extraction Detailed references for noLLM (CSS, XPath) vs. LLMbased strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, youll find code samples you can copypaste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star  Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether its a small fix, a big feature, or better docscontributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyonestudents, researchers, entrepreneurs, data scientiststo access, parse, and shape the worlds data with speed, costefficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Lets keep building an open, democratic approach to data extraction and AI together. Happy Crawling!  Unclecode, Founder  Maintainer of Crawl4AI  Crawl4AI: OpenSource LLMFriendly Web Crawler  Scraper Crawl4AI is the 1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazingfast, AIready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for realtime performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. Note: If youre looking for the old documentation, you can access it here. Quick Start Heres a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main():  Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler:  Run the crawler on a URL result  await crawler.arun(urlhttps:crawl4ai.com)  Print the extracted content print(result.markdown)  Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a featurerich crawler and scraper that aims to: 1. Generate Clean Markdown: Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction: Parse repeated patterns with CSS, XPath, or LLMbased extraction. 3. Advanced Browser Control: Hooks, proxies, stealth modes, session reusefinegrained control. 4. High Performance: Parallel crawling, chunkbased extraction, realtime use cases. 5. Open Source: No forced API keys, no paywallseveryone can access their data. Core Philosophies:  Democratize Data: Free to use, transparent, and highly configurable.  LLM Friendly: Minimally processed, wellstructured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, weve organized our docs into clear sections: Setup  Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A handson introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on singlepage crawling, advanced browsercrawler parameters, content filtering, and caching. Advanced Explore link  media handling, lazy loading, hooking  authentication, proxies, session management, and more. Extraction Detailed references for noLLM (CSS, XPath) vs. LLMbased strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, youll find code samples you can copypaste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star  Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests: Whether its a small fix, a big feature, or better docscontributions are always welcome. Join Discord: Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word: Mention Crawl4AI in your blog posts, talks, or on social media. Our mission: to empower everyonestudents, researchers, entrepreneurs, data scientiststo access, parse, and shape the worlds data with speed, costefficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Lets keep building an open, democratic approach to data extraction and AI together. Happy Crawling!  Unclecode, Founder  Maintainer of Crawl4AI import asyncio from crawl4ai import AsyncWebCrawler async def main():  Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler:  Run the crawler on a URL result  await crawler.arun(urlhttps:crawl4ai.com)  Print the extracted content print(result.markdown)  Run the async main function asyncio.run(main()) Search Type to start searching Search Type to start searching Search Type to start searching Search Type to start searching

---

## 文档 2 <span id='doc-2'></span>

### 元数据

- **来源URL**: https://crawl4ai.com/mkdocs/core/quickstart/
- **字数统计**: 59073
- **抓取时间**: 2025-02-13 17:30:59

### 内容

> Getting Started with Crawl4AI Welcome to Crawl4AI, an opensource LLMfriendly Web Crawler  Scraper. In this tutorial, youll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how its influenced by content filters). Experiment with a simple CSSbased extraction strategy. See a glimpse of LLMbased extraction (including opensource and closedsource model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler. Configurable browser and run settings via BrowserConfig and CrawlerRunConfig. Automatic HTMLtoMarkdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLMbased or traditional CSSXPathbased). By the end of this guide, youll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses Load More buttons or JavaScript updates. 2. Your First Crawl Heres a minimal Python script that creates an AsyncWebCrawler, fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:example.com) print(result.markdown:300)  Print first 300 chars if name  main: asyncio.run(main()) Whats happening?  AsyncWebCrawler launches a headless browser (Chromium by default).  It fetches https:example.com.  Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) Crawl4AIs crawler can be heavily customized using two main classes: 1. BrowserConfig: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2. CrawlerRunConfig: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browserconf  BrowserConfig(headlessTrue)  or False to see the browser runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS ) async with AsyncWebCrawler(configbrowserconf) as crawler: result  await crawler.arun( urlhttps:example.com, configrunconf ) print(result.markdown) if name  main: asyncio.run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED. So to have fresh content, you need to set it to CacheMode.BYPASS Well explore more advanced config in later tutorials (like enabling proxies, PDF output, multitab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter. result.markdown: The direct HTMLtoMarkdown conversion. result.markdown.fitmarkdown: The same content after applying any configured content filter (e.g., PruningContentFilter). Example: Using a Filter with DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.contentfilterstrategy import PruningContentFilter from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator mdgenerator  DefaultMarkdownGenerator( contentfilterPruningContentFilter(threshold0.4, thresholdtypefixed) ) config  CrawlerRunConfig( cachemodeCacheMode.BYPASS, markdowngeneratormdgenerator ) async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:news.ycombinator.com, configconfig) print(Raw Markdown length:, len(result.markdown.rawmarkdown)) print(Fit Markdown length:, len(result.markdown.fitmarkdown)) Note: If you do not specify a content filter or markdown generator, youll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. Well dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSSbased) Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSSbased example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a onetime cost that gives you a reusable schema for fast, LLMfree extractions: from crawl4ai.extractionstrategy import JsonCssExtractionStrategy  Generate a schema (onetime cost) html  div classproducth2Gaming Laptoph2span classprice999.99spandiv  Using OpenAI (requires API token) schema  JsonCssExtractionStrategy.generateschema( html, llmprovideropenaigpt4o,  Default provider apitokenyouropenaitoken  Required for OpenAI )  Or using Ollama (open source, no token needed) schema  JsonCssExtractionStrategy.generateschema( html, llmproviderollamallama3.3,  Open source alternative apitokenNone  Not needed for Ollama )  Use the schema for fast, repeated extractions strategy  JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see NoLLM Extraction Strategies. Heres a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): schema   name: Example Items, baseSelector: div.item, fields:  name: title, selector: h2, type: text, name: link, selector: a, type: attribute, attribute: href   rawhtml  div classitemh2Item 1h2a hrefhttps:example.comitem1Link 1adiv async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlraw:  rawhtml, configCrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema) ) )  The JSON output is stored in extractedcontent data  json.loads(result.extractedcontent) print(data) if name  main: asyncio.run(main()) Why is this helpful?  Great for repetitive page structures (e.g., item listings, articles).  No AI usage or costs.  The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw:. 6. Simple Data Extraction (LLMbased) For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports opensource or closedsource providers: OpenSource Models (e.g., ollamallama3.3, notoken) OpenAI Models (e.g., openaigpt4, requires apitoken) Or any provider supported by the underlying library Below is an example using opensource style (no token) and closedsource: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extractionstrategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): modelname: str  Field(., descriptionName of the OpenAI model.) inputfee: str  Field(., descriptionFee for input token for the OpenAI model.) outputfee: str  Field( ., descriptionFee for output token for the OpenAI model. ) async def extractstructureddatausingllm( provider: str, apitoken: str  None, extraheaders: Dictstr, str  None ): print(fn Extracting Structured Data with provider ) if apitoken is None and provider ! ollama: print(fAPI token is required for provider. Skipping this example.) return browserconfig  BrowserConfig(headlessTrue) extraargs  temperature: 0, topp: 0.9, maxtokens: 2000 if extraheaders: extraargsextraheaders  extraheaders crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, wordcountthreshold1, pagetimeout80000, extractionstrategyLLMExtractionStrategy( providerprovider, apitokenapitoken, schemaOpenAIModelFee.modeljsonschema(), extractiontypeschema, instructionFrom the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content., extraargsextraargs, ), ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:openai.comapipricing, configcrawlerconfig ) print(result.extractedcontent) if name  main:  Use ollama with llama3.3  asyncio.run(  extractstructureddatausingllm(  providerollamallama3.3, apitokennotoken  )  ) asyncio.run( extractstructureddatausingllm( provideropenaigpt4o, apitokenos.getenv(OPENAIAPIKEY) ) ) Whats happening?  We define a Pydantic schema (PricingInfo) describing the fields we want.  The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON.  Depending on the provider and apitoken, you can use local models or a remote API. 7. MultiURL Concurrency (Preview) If you need to crawl multiple URLs in parallel, you can use arunmany(). By default, Crawl4AI employs a MemoryAdaptiveDispatcher, automatically adjusting concurrency based on system resources. Heres a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quickparallelexample(): urls   https:example.compage1, https:example.compage2, https:example.compage3  runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS, streamTrue  Enable streaming mode ) async with AsyncWebCrawler() as crawler:  Stream results as they complete async for result in await crawler.arunmany(urls, configrunconf): if result.success: print(fOK result.url, length: len(result.markdownv2.rawmarkdown)) else: print(fERROR result.url  result.errormessage)  Or get all results at once (default behavior) runconf  runconf.clone(streamFalse) results  await crawler.arunmany(urls, configrunconf) for res in results: if res.success: print(fOK res.url, length: len(res.markdownv2.rawmarkdown)) else: print(fERROR res.url  res.errormessage) if name  main: asyncio.run(quickparallelexample()) The example above shows two ways to handle multiple URLs: 1. Streaming mode (streamTrue): Process results as they become available using async for 2. Batch mode (streamFalse): Wait for all results to complete For more advanced concurrency (e.g., a semaphorebased approach, adaptive memory usage throttling, or customized rate limiting), see Advanced MultiURL Crawling. 8. Dynamic Content Example Some sites require multiple page clicks or dynamic JavaScript updates. Below is an example showing how to click a Next Page button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def extractstructureddatausingcssextractor(): print(n Using JsonCssExtractionStrategy for Fast Structured Output ) schema   name: KidoCode Courses, baseSelector: section.chargemethodology .wtabcontent  div, fields:   name: sectiontitle, selector: h3.heading50, type: text, ,  name: sectiondescription, selector: .chargecontent, type: text, ,  name: coursename, selector: .textblock93, type: text, ,  name: coursedescription, selector: .coursecontenttext, type: text, ,  name: courseicon, selector: .image92, type: attribute, attribute: src, , ,  browserconfig  BrowserConfig(headlessTrue, javascriptenabledTrue) jsclicktabs   (async ()   const tabs  document.querySelectorAll(section.chargemethodology .tabsmenu3  div) for(let tab of tabs)  tab.scrollIntoView() tab.click() await new Promise(r  setTimeout(r, 500))  )()  crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), jscodejsclicktabs, ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:www.kidocode.comdegreestechnology, configcrawlerconfig ) companies  json.loads(result.extractedcontent) print(fSuccessfully extracted len(companies) companies) print(json.dumps(companies0, indent2)) async def main(): await extractstructureddatausingcssextractor() if name  main: asyncio.run(main()) Key Points: BrowserConfig(headlessFalse): We want to watch it click Next Page. CrawlerRunConfig(.): We specify the extraction strategy, pass sessionid to reuse the same page. jscode and waitfor are used for subsequent pages (page  0) to click the Next button and wait for new commits to load. jsonlyTrue indicates were not renavigating but continuing the existing session. Finally, we call killsession() to clean up the page and browser session. 9. Next Steps Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If youre ready for more, check out: Installation: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks  Auth: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management: Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AIdriven extraction flows. Happy crawling! Getting Started with Crawl4AI Welcome to Crawl4AI, an opensource LLMfriendly Web Crawler  Scraper. In this tutorial, youll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how its influenced by content filters). Experiment with a simple CSSbased extraction strategy. See a glimpse of LLMbased extraction (including opensource and closedsource model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler. Configurable browser and run settings via BrowserConfig and CrawlerRunConfig. Automatic HTMLtoMarkdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLMbased or traditional CSSXPathbased). By the end of this guide, youll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses Load More buttons or JavaScript updates. 2. Your First Crawl Heres a minimal Python script that creates an AsyncWebCrawler, fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:example.com) print(result.markdown:300)  Print first 300 chars if name  main: asyncio.run(main()) Whats happening?  AsyncWebCrawler launches a headless browser (Chromium by default).  It fetches https:example.com.  Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) Crawl4AIs crawler can be heavily customized using two main classes: 1. BrowserConfig: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2. CrawlerRunConfig: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browserconf  BrowserConfig(headlessTrue)  or False to see the browser runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS ) async with AsyncWebCrawler(configbrowserconf) as crawler: result  await crawler.arun( urlhttps:example.com, configrunconf ) print(result.markdown) if name  main: asyncio.run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED. So to have fresh content, you need to set it to CacheMode.BYPASS Well explore more advanced config in later tutorials (like enabling proxies, PDF output, multitab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter. result.markdown: The direct HTMLtoMarkdown conversion. result.markdown.fitmarkdown: The same content after applying any configured content filter (e.g., PruningContentFilter). Example: Using a Filter with DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.contentfilterstrategy import PruningContentFilter from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator mdgenerator  DefaultMarkdownGenerator( contentfilterPruningContentFilter(threshold0.4, thresholdtypefixed) ) config  CrawlerRunConfig( cachemodeCacheMode.BYPASS, markdowngeneratormdgenerator ) async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:news.ycombinator.com, configconfig) print(Raw Markdown length:, len(result.markdown.rawmarkdown)) print(Fit Markdown length:, len(result.markdown.fitmarkdown)) Note: If you do not specify a content filter or markdown generator, youll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. Well dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSSbased) Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSSbased example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a onetime cost that gives you a reusable schema for fast, LLMfree extractions: from crawl4ai.extractionstrategy import JsonCssExtractionStrategy  Generate a schema (onetime cost) html  div classproducth2Gaming Laptoph2span classprice999.99spandiv  Using OpenAI (requires API token) schema  JsonCssExtractionStrategy.generateschema( html, llmprovideropenaigpt4o,  Default provider apitokenyouropenaitoken  Required for OpenAI )  Or using Ollama (open source, no token needed) schema  JsonCssExtractionStrategy.generateschema( html, llmproviderollamallama3.3,  Open source alternative apitokenNone  Not needed for Ollama )  Use the schema for fast, repeated extractions strategy  JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see NoLLM Extraction Strategies. Heres a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): schema   name: Example Items, baseSelector: div.item, fields:  name: title, selector: h2, type: text, name: link, selector: a, type: attribute, attribute: href   rawhtml  div classitemh2Item 1h2a hrefhttps:example.comitem1Link 1adiv async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlraw:  rawhtml, configCrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema) ) )  The JSON output is stored in extractedcontent data  json.loads(result.extractedcontent) print(data) if name  main: asyncio.run(main()) Why is this helpful?  Great for repetitive page structures (e.g., item listings, articles).  No AI usage or costs.  The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw:. 6. Simple Data Extraction (LLMbased) For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports opensource or closedsource providers: OpenSource Models (e.g., ollamallama3.3, notoken) OpenAI Models (e.g., openaigpt4, requires apitoken) Or any provider supported by the underlying library Below is an example using opensource style (no token) and closedsource: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extractionstrategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): modelname: str  Field(., descriptionName of the OpenAI model.) inputfee: str  Field(., descriptionFee for input token for the OpenAI model.) outputfee: str  Field( ., descriptionFee for output token for the OpenAI model. ) async def extractstructureddatausingllm( provider: str, apitoken: str  None, extraheaders: Dictstr, str  None ): print(fn Extracting Structured Data with provider ) if apitoken is None and provider ! ollama: print(fAPI token is required for provider. Skipping this example.) return browserconfig  BrowserConfig(headlessTrue) extraargs  temperature: 0, topp: 0.9, maxtokens: 2000 if extraheaders: extraargsextraheaders  extraheaders crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, wordcountthreshold1, pagetimeout80000, extractionstrategyLLMExtractionStrategy( providerprovider, apitokenapitoken, schemaOpenAIModelFee.modeljsonschema(), extractiontypeschema, instructionFrom the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content., extraargsextraargs, ), ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:openai.comapipricing, configcrawlerconfig ) print(result.extractedcontent) if name  main:  Use ollama with llama3.3  asyncio.run(  extractstructureddatausingllm(  providerollamallama3.3, apitokennotoken  )  ) asyncio.run( extractstructureddatausingllm( provideropenaigpt4o, apitokenos.getenv(OPENAIAPIKEY) ) ) Whats happening?  We define a Pydantic schema (PricingInfo) describing the fields we want.  The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON.  Depending on the provider and apitoken, you can use local models or a remote API. 7. MultiURL Concurrency (Preview) If you need to crawl multiple URLs in parallel, you can use arunmany(). By default, Crawl4AI employs a MemoryAdaptiveDispatcher, automatically adjusting concurrency based on system resources. Heres a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quickparallelexample(): urls   https:example.compage1, https:example.compage2, https:example.compage3  runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS, streamTrue  Enable streaming mode ) async with AsyncWebCrawler() as crawler:  Stream results as they complete async for result in await crawler.arunmany(urls, configrunconf): if result.success: print(fOK result.url, length: len(result.markdownv2.rawmarkdown)) else: print(fERROR result.url  result.errormessage)  Or get all results at once (default behavior) runconf  runconf.clone(streamFalse) results  await crawler.arunmany(urls, configrunconf) for res in results: if res.success: print(fOK res.url, length: len(res.markdownv2.rawmarkdown)) else: print(fERROR res.url  res.errormessage) if name  main: asyncio.run(quickparallelexample()) The example above shows two ways to handle multiple URLs: 1. Streaming mode (streamTrue): Process results as they become available using async for 2. Batch mode (streamFalse): Wait for all results to complete For more advanced concurrency (e.g., a semaphorebased approach, adaptive memory usage throttling, or customized rate limiting), see Advanced MultiURL Crawling. 8. Dynamic Content Example Some sites require multiple page clicks or dynamic JavaScript updates. Below is an example showing how to click a Next Page button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def extractstructureddatausingcssextractor(): print(n Using JsonCssExtractionStrategy for Fast Structured Output ) schema   name: KidoCode Courses, baseSelector: section.chargemethodology .wtabcontent  div, fields:   name: sectiontitle, selector: h3.heading50, type: text, ,  name: sectiondescription, selector: .chargecontent, type: text, ,  name: coursename, selector: .textblock93, type: text, ,  name: coursedescription, selector: .coursecontenttext, type: text, ,  name: courseicon, selector: .image92, type: attribute, attribute: src, , ,  browserconfig  BrowserConfig(headlessTrue, javascriptenabledTrue) jsclicktabs   (async ()   const tabs  document.querySelectorAll(section.chargemethodology .tabsmenu3  div) for(let tab of tabs)  tab.scrollIntoView() tab.click() await new Promise(r  setTimeout(r, 500))  )()  crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), jscodejsclicktabs, ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:www.kidocode.comdegreestechnology, configcrawlerconfig ) companies  json.loads(result.extractedcontent) print(fSuccessfully extracted len(companies) companies) print(json.dumps(companies0, indent2)) async def main(): await extractstructureddatausingcssextractor() if name  main: asyncio.run(main()) Key Points: BrowserConfig(headlessFalse): We want to watch it click Next Page. CrawlerRunConfig(.): We specify the extraction strategy, pass sessionid to reuse the same page. jscode and waitfor are used for subsequent pages (page  0) to click the Next button and wait for new commits to load. jsonlyTrue indicates were not renavigating but continuing the existing session. Finally, we call killsession() to clean up the page and browser session. 9. Next Steps Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If youre ready for more, check out: Installation: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks  Auth: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management: Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AIdriven extraction flows. Happy crawling! Getting Started with Crawl4AI Welcome to Crawl4AI, an opensource LLMfriendly Web Crawler  Scraper. In this tutorial, youll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how its influenced by content filters). Experiment with a simple CSSbased extraction strategy. See a glimpse of LLMbased extraction (including opensource and closedsource model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler. Configurable browser and run settings via BrowserConfig and CrawlerRunConfig. Automatic HTMLtoMarkdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLMbased or traditional CSSXPathbased). By the end of this guide, youll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses Load More buttons or JavaScript updates. 2. Your First Crawl Heres a minimal Python script that creates an AsyncWebCrawler, fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:example.com) print(result.markdown:300)  Print first 300 chars if name  main: asyncio.run(main()) Whats happening?  AsyncWebCrawler launches a headless browser (Chromium by default).  It fetches https:example.com.  Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) Crawl4AIs crawler can be heavily customized using two main classes: 1. BrowserConfig: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2. CrawlerRunConfig: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browserconf  BrowserConfig(headlessTrue)  or False to see the browser runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS ) async with AsyncWebCrawler(configbrowserconf) as crawler: result  await crawler.arun( urlhttps:example.com, configrunconf ) print(result.markdown) if name  main: asyncio.run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED. So to have fresh content, you need to set it to CacheMode.BYPASS Well explore more advanced config in later tutorials (like enabling proxies, PDF output, multitab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter. result.markdown: The direct HTMLtoMarkdown conversion. result.markdown.fitmarkdown: The same content after applying any configured content filter (e.g., PruningContentFilter). Example: Using a Filter with DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.contentfilterstrategy import PruningContentFilter from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator mdgenerator  DefaultMarkdownGenerator( contentfilterPruningContentFilter(threshold0.4, thresholdtypefixed) ) config  CrawlerRunConfig( cachemodeCacheMode.BYPASS, markdowngeneratormdgenerator ) async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:news.ycombinator.com, configconfig) print(Raw Markdown length:, len(result.markdown.rawmarkdown)) print(Fit Markdown length:, len(result.markdown.fitmarkdown)) Note: If you do not specify a content filter or markdown generator, youll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. Well dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSSbased) Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSSbased example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a onetime cost that gives you a reusable schema for fast, LLMfree extractions: from crawl4ai.extractionstrategy import JsonCssExtractionStrategy  Generate a schema (onetime cost) html  div classproducth2Gaming Laptoph2span classprice999.99spandiv  Using OpenAI (requires API token) schema  JsonCssExtractionStrategy.generateschema( html, llmprovideropenaigpt4o,  Default provider apitokenyouropenaitoken  Required for OpenAI )  Or using Ollama (open source, no token needed) schema  JsonCssExtractionStrategy.generateschema( html, llmproviderollamallama3.3,  Open source alternative apitokenNone  Not needed for Ollama )  Use the schema for fast, repeated extractions strategy  JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see NoLLM Extraction Strategies. Heres a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): schema   name: Example Items, baseSelector: div.item, fields:  name: title, selector: h2, type: text, name: link, selector: a, type: attribute, attribute: href   rawhtml  div classitemh2Item 1h2a hrefhttps:example.comitem1Link 1adiv async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlraw:  rawhtml, configCrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema) ) )  The JSON output is stored in extractedcontent data  json.loads(result.extractedcontent) print(data) if name  main: asyncio.run(main()) Why is this helpful?  Great for repetitive page structures (e.g., item listings, articles).  No AI usage or costs.  The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw:. 6. Simple Data Extraction (LLMbased) For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports opensource or closedsource providers: OpenSource Models (e.g., ollamallama3.3, notoken) OpenAI Models (e.g., openaigpt4, requires apitoken) Or any provider supported by the underlying library Below is an example using opensource style (no token) and closedsource: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extractionstrategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): modelname: str  Field(., descriptionName of the OpenAI model.) inputfee: str  Field(., descriptionFee for input token for the OpenAI model.) outputfee: str  Field( ., descriptionFee for output token for the OpenAI model. ) async def extractstructureddatausingllm( provider: str, apitoken: str  None, extraheaders: Dictstr, str  None ): print(fn Extracting Structured Data with provider ) if apitoken is None and provider ! ollama: print(fAPI token is required for provider. Skipping this example.) return browserconfig  BrowserConfig(headlessTrue) extraargs  temperature: 0, topp: 0.9, maxtokens: 2000 if extraheaders: extraargsextraheaders  extraheaders crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, wordcountthreshold1, pagetimeout80000, extractionstrategyLLMExtractionStrategy( providerprovider, apitokenapitoken, schemaOpenAIModelFee.modeljsonschema(), extractiontypeschema, instructionFrom the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content., extraargsextraargs, ), ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:openai.comapipricing, configcrawlerconfig ) print(result.extractedcontent) if name  main:  Use ollama with llama3.3  asyncio.run(  extractstructureddatausingllm(  providerollamallama3.3, apitokennotoken  )  ) asyncio.run( extractstructureddatausingllm( provideropenaigpt4o, apitokenos.getenv(OPENAIAPIKEY) ) ) Whats happening?  We define a Pydantic schema (PricingInfo) describing the fields we want.  The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON.  Depending on the provider and apitoken, you can use local models or a remote API. 7. MultiURL Concurrency (Preview) If you need to crawl multiple URLs in parallel, you can use arunmany(). By default, Crawl4AI employs a MemoryAdaptiveDispatcher, automatically adjusting concurrency based on system resources. Heres a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quickparallelexample(): urls   https:example.compage1, https:example.compage2, https:example.compage3  runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS, streamTrue  Enable streaming mode ) async with AsyncWebCrawler() as crawler:  Stream results as they complete async for result in await crawler.arunmany(urls, configrunconf): if result.success: print(fOK result.url, length: len(result.markdownv2.rawmarkdown)) else: print(fERROR result.url  result.errormessage)  Or get all results at once (default behavior) runconf  runconf.clone(streamFalse) results  await crawler.arunmany(urls, configrunconf) for res in results: if res.success: print(fOK res.url, length: len(res.markdownv2.rawmarkdown)) else: print(fERROR res.url  res.errormessage) if name  main: asyncio.run(quickparallelexample()) The example above shows two ways to handle multiple URLs: 1. Streaming mode (streamTrue): Process results as they become available using async for 2. Batch mode (streamFalse): Wait for all results to complete For more advanced concurrency (e.g., a semaphorebased approach, adaptive memory usage throttling, or customized rate limiting), see Advanced MultiURL Crawling. 8. Dynamic Content Example Some sites require multiple page clicks or dynamic JavaScript updates. Below is an example showing how to click a Next Page button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def extractstructureddatausingcssextractor(): print(n Using JsonCssExtractionStrategy for Fast Structured Output ) schema   name: KidoCode Courses, baseSelector: section.chargemethodology .wtabcontent  div, fields:   name: sectiontitle, selector: h3.heading50, type: text, ,  name: sectiondescription, selector: .chargecontent, type: text, ,  name: coursename, selector: .textblock93, type: text, ,  name: coursedescription, selector: .coursecontenttext, type: text, ,  name: courseicon, selector: .image92, type: attribute, attribute: src, , ,  browserconfig  BrowserConfig(headlessTrue, javascriptenabledTrue) jsclicktabs   (async ()   const tabs  document.querySelectorAll(section.chargemethodology .tabsmenu3  div) for(let tab of tabs)  tab.scrollIntoView() tab.click() await new Promise(r  setTimeout(r, 500))  )()  crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), jscodejsclicktabs, ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:www.kidocode.comdegreestechnology, configcrawlerconfig ) companies  json.loads(result.extractedcontent) print(fSuccessfully extracted len(companies) companies) print(json.dumps(companies0, indent2)) async def main(): await extractstructureddatausingcssextractor() if name  main: asyncio.run(main()) Key Points: BrowserConfig(headlessFalse): We want to watch it click Next Page. CrawlerRunConfig(.): We specify the extraction strategy, pass sessionid to reuse the same page. jscode and waitfor are used for subsequent pages (page  0) to click the Next button and wait for new commits to load. jsonlyTrue indicates were not renavigating but continuing the existing session. Finally, we call killsession() to clean up the page and browser session. 9. Next Steps Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If youre ready for more, check out: Installation: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks  Auth: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management: Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AIdriven extraction flows. Happy crawling! Getting Started with Crawl4AI Welcome to Crawl4AI, an opensource LLMfriendly Web Crawler  Scraper. In this tutorial, youll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how its influenced by content filters). Experiment with a simple CSSbased extraction strategy. See a glimpse of LLMbased extraction (including opensource and closedsource model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler. Configurable browser and run settings via BrowserConfig and CrawlerRunConfig. Automatic HTMLtoMarkdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLMbased or traditional CSSXPathbased). By the end of this guide, youll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses Load More buttons or JavaScript updates. 2. Your First Crawl Heres a minimal Python script that creates an AsyncWebCrawler, fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:example.com) print(result.markdown:300)  Print first 300 chars if name  main: asyncio.run(main()) Whats happening?  AsyncWebCrawler launches a headless browser (Chromium by default).  It fetches https:example.com.  Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) Crawl4AIs crawler can be heavily customized using two main classes: 1. BrowserConfig: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2. CrawlerRunConfig: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browserconf  BrowserConfig(headlessTrue)  or False to see the browser runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS ) async with AsyncWebCrawler(configbrowserconf) as crawler: result  await crawler.arun( urlhttps:example.com, configrunconf ) print(result.markdown) if name  main: asyncio.run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED. So to have fresh content, you need to set it to CacheMode.BYPASS Well explore more advanced config in later tutorials (like enabling proxies, PDF output, multitab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter. result.markdown: The direct HTMLtoMarkdown conversion. result.markdown.fitmarkdown: The same content after applying any configured content filter (e.g., PruningContentFilter). Example: Using a Filter with DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.contentfilterstrategy import PruningContentFilter from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator mdgenerator  DefaultMarkdownGenerator( contentfilterPruningContentFilter(threshold0.4, thresholdtypefixed) ) config  CrawlerRunConfig( cachemodeCacheMode.BYPASS, markdowngeneratormdgenerator ) async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:news.ycombinator.com, configconfig) print(Raw Markdown length:, len(result.markdown.rawmarkdown)) print(Fit Markdown length:, len(result.markdown.fitmarkdown)) Note: If you do not specify a content filter or markdown generator, youll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. Well dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSSbased) Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSSbased example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a onetime cost that gives you a reusable schema for fast, LLMfree extractions: from crawl4ai.extractionstrategy import JsonCssExtractionStrategy  Generate a schema (onetime cost) html  div classproducth2Gaming Laptoph2span classprice999.99spandiv  Using OpenAI (requires API token) schema  JsonCssExtractionStrategy.generateschema( html, llmprovideropenaigpt4o,  Default provider apitokenyouropenaitoken  Required for OpenAI )  Or using Ollama (open source, no token needed) schema  JsonCssExtractionStrategy.generateschema( html, llmproviderollamallama3.3,  Open source alternative apitokenNone  Not needed for Ollama )  Use the schema for fast, repeated extractions strategy  JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see NoLLM Extraction Strategies. Heres a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): schema   name: Example Items, baseSelector: div.item, fields:  name: title, selector: h2, type: text, name: link, selector: a, type: attribute, attribute: href   rawhtml  div classitemh2Item 1h2a hrefhttps:example.comitem1Link 1adiv async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlraw:  rawhtml, configCrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema) ) )  The JSON output is stored in extractedcontent data  json.loads(result.extractedcontent) print(data) if name  main: asyncio.run(main()) Why is this helpful?  Great for repetitive page structures (e.g., item listings, articles).  No AI usage or costs.  The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw:. 6. Simple Data Extraction (LLMbased) For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports opensource or closedsource providers: OpenSource Models (e.g., ollamallama3.3, notoken) OpenAI Models (e.g., openaigpt4, requires apitoken) Or any provider supported by the underlying library Below is an example using opensource style (no token) and closedsource: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extractionstrategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): modelname: str  Field(., descriptionName of the OpenAI model.) inputfee: str  Field(., descriptionFee for input token for the OpenAI model.) outputfee: str  Field( ., descriptionFee for output token for the OpenAI model. ) async def extractstructureddatausingllm( provider: str, apitoken: str  None, extraheaders: Dictstr, str  None ): print(fn Extracting Structured Data with provider ) if apitoken is None and provider ! ollama: print(fAPI token is required for provider. Skipping this example.) return browserconfig  BrowserConfig(headlessTrue) extraargs  temperature: 0, topp: 0.9, maxtokens: 2000 if extraheaders: extraargsextraheaders  extraheaders crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, wordcountthreshold1, pagetimeout80000, extractionstrategyLLMExtractionStrategy( providerprovider, apitokenapitoken, schemaOpenAIModelFee.modeljsonschema(), extractiontypeschema, instructionFrom the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content., extraargsextraargs, ), ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:openai.comapipricing, configcrawlerconfig ) print(result.extractedcontent) if name  main:  Use ollama with llama3.3  asyncio.run(  extractstructureddatausingllm(  providerollamallama3.3, apitokennotoken  )  ) asyncio.run( extractstructureddatausingllm( provideropenaigpt4o, apitokenos.getenv(OPENAIAPIKEY) ) ) Whats happening?  We define a Pydantic schema (PricingInfo) describing the fields we want.  The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON.  Depending on the provider and apitoken, you can use local models or a remote API. 7. MultiURL Concurrency (Preview) If you need to crawl multiple URLs in parallel, you can use arunmany(). By default, Crawl4AI employs a MemoryAdaptiveDispatcher, automatically adjusting concurrency based on system resources. Heres a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quickparallelexample(): urls   https:example.compage1, https:example.compage2, https:example.compage3  runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS, streamTrue  Enable streaming mode ) async with AsyncWebCrawler() as crawler:  Stream results as they complete async for result in await crawler.arunmany(urls, configrunconf): if result.success: print(fOK result.url, length: len(result.markdownv2.rawmarkdown)) else: print(fERROR result.url  result.errormessage)  Or get all results at once (default behavior) runconf  runconf.clone(streamFalse) results  await crawler.arunmany(urls, configrunconf) for res in results: if res.success: print(fOK res.url, length: len(res.markdownv2.rawmarkdown)) else: print(fERROR res.url  res.errormessage) if name  main: asyncio.run(quickparallelexample()) The example above shows two ways to handle multiple URLs: 1. Streaming mode (streamTrue): Process results as they become available using async for 2. Batch mode (streamFalse): Wait for all results to complete For more advanced concurrency (e.g., a semaphorebased approach, adaptive memory usage throttling, or customized rate limiting), see Advanced MultiURL Crawling. 8. Dynamic Content Example Some sites require multiple page clicks or dynamic JavaScript updates. Below is an example showing how to click a Next Page button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def extractstructureddatausingcssextractor(): print(n Using JsonCssExtractionStrategy for Fast Structured Output ) schema   name: KidoCode Courses, baseSelector: section.chargemethodology .wtabcontent  div, fields:   name: sectiontitle, selector: h3.heading50, type: text, ,  name: sectiondescription, selector: .chargecontent, type: text, ,  name: coursename, selector: .textblock93, type: text, ,  name: coursedescription, selector: .coursecontenttext, type: text, ,  name: courseicon, selector: .image92, type: attribute, attribute: src, , ,  browserconfig  BrowserConfig(headlessTrue, javascriptenabledTrue) jsclicktabs   (async ()   const tabs  document.querySelectorAll(section.chargemethodology .tabsmenu3  div) for(let tab of tabs)  tab.scrollIntoView() tab.click() await new Promise(r  setTimeout(r, 500))  )()  crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), jscodejsclicktabs, ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:www.kidocode.comdegreestechnology, configcrawlerconfig ) companies  json.loads(result.extractedcontent) print(fSuccessfully extracted len(companies) companies) print(json.dumps(companies0, indent2)) async def main(): await extractstructureddatausingcssextractor() if name  main: asyncio.run(main()) Key Points: BrowserConfig(headlessFalse): We want to watch it click Next Page. CrawlerRunConfig(.): We specify the extraction strategy, pass sessionid to reuse the same page. jscode and waitfor are used for subsequent pages (page  0) to click the Next button and wait for new commits to load. jsonlyTrue indicates were not renavigating but continuing the existing session. Finally, we call killsession() to clean up the page and browser session. 9. Next Steps Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If youre ready for more, check out: Installation: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks  Auth: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management: Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AIdriven extraction flows. Happy crawling! import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:example.com) print(result.markdown:300)  Print first 300 chars if name  main: asyncio.run(main()) import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browserconf  BrowserConfig(headlessTrue)  or False to see the browser runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS ) async with AsyncWebCrawler(configbrowserconf) as crawler: result  await crawler.arun( urlhttps:example.com, configrunconf ) print(result.markdown) if name  main: asyncio.run(main()) from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.contentfilterstrategy import PruningContentFilter from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator mdgenerator  DefaultMarkdownGenerator( contentfilterPruningContentFilter(threshold0.4, thresholdtypefixed) ) config  CrawlerRunConfig( cachemodeCacheMode.BYPASS, markdowngeneratormdgenerator ) async with AsyncWebCrawler() as crawler: result  await crawler.arun(https:news.ycombinator.com, configconfig) print(Raw Markdown length:, len(result.markdown.rawmarkdown)) print(Fit Markdown length:, len(result.markdown.fitmarkdown)) from crawl4ai.extractionstrategy import JsonCssExtractionStrategy  Generate a schema (onetime cost) html  div classproducth2Gaming Laptoph2span classprice999.99spandiv  Using OpenAI (requires API token) schema  JsonCssExtractionStrategy.generateschema( html, llmprovideropenaigpt4o,  Default provider apitokenyouropenaitoken  Required for OpenAI )  Or using Ollama (open source, no token needed) schema  JsonCssExtractionStrategy.generateschema( html, llmproviderollamallama3.3,  Open source alternative apitokenNone  Not needed for Ollama )  Use the schema for fast, repeated extractions strategy  JsonCssExtractionStrategy(schema) import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): schema   name: Example Items, baseSelector: div.item, fields:  name: title, selector: h2, type: text, name: link, selector: a, type: attribute, attribute: href   rawhtml  div classitemh2Item 1h2a hrefhttps:example.comitem1Link 1adiv async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlraw:  rawhtml, configCrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema) ) )  The JSON output is stored in extractedcontent data  json.loads(result.extractedcontent) print(data) if name  main: asyncio.run(main()) import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extractionstrategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): modelname: str  Field(., descriptionName of the OpenAI model.) inputfee: str  Field(., descriptionFee for input token for the OpenAI model.) outputfee: str  Field( ., descriptionFee for output token for the OpenAI model. ) async def extractstructureddatausingllm( provider: str, apitoken: str  None, extraheaders: Dictstr, str  None ): print(fn Extracting Structured Data with provider ) if apitoken is None and provider ! ollama: print(fAPI token is required for provider. Skipping this example.) return browserconfig  BrowserConfig(headlessTrue) extraargs  temperature: 0, topp: 0.9, maxtokens: 2000 if extraheaders: extraargsextraheaders  extraheaders crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, wordcountthreshold1, pagetimeout80000, extractionstrategyLLMExtractionStrategy( providerprovider, apitokenapitoken, schemaOpenAIModelFee.modeljsonschema(), extractiontypeschema, instructionFrom the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content., extraargsextraargs, ), ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:openai.comapipricing, configcrawlerconfig ) print(result.extractedcontent) if name  main:  Use ollama with llama3.3  asyncio.run(  extractstructureddatausingllm(  providerollamallama3.3, apitokennotoken  )  ) asyncio.run( extractstructureddatausingllm( provideropenaigpt4o, apitokenos.getenv(OPENAIAPIKEY) ) ) import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quickparallelexample(): urls   https:example.compage1, https:example.compage2, https:example.compage3  runconf  CrawlerRunConfig( cachemodeCacheMode.BYPASS, streamTrue  Enable streaming mode ) async with AsyncWebCrawler() as crawler:  Stream results as they complete async for result in await crawler.arunmany(urls, configrunconf): if result.success: print(fOK result.url, length: len(result.markdownv2.rawmarkdown)) else: print(fERROR result.url  result.errormessage)  Or get all results at once (default behavior) runconf  runconf.clone(streamFalse) results  await crawler.arunmany(urls, configrunconf) for res in results: if res.success: print(fOK res.url, length: len(res.markdownv2.rawmarkdown)) else: print(fERROR res.url  res.errormessage) if name  main: asyncio.run(quickparallelexample()) import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def extractstructureddatausingcssextractor(): print(n Using JsonCssExtractionStrategy for Fast Structured Output ) schema   name: KidoCode Courses, baseSelector: section.chargemethodology .wtabcontent  div, fields:   name: sectiontitle, selector: h3.heading50, type: text, ,  name: sectiondescription, selector: .chargecontent, type: text, ,  name: coursename, selector: .textblock93, type: text, ,  name: coursedescription, selector: .coursecontenttext, type: text, ,  name: courseicon, selector: .image92, type: attribute, attribute: src, , ,  browserconfig  BrowserConfig(headlessTrue, javascriptenabledTrue) jsclicktabs   (async ()   const tabs  document.querySelectorAll(section.chargemethodology .tabsmenu3  div) for(let tab of tabs)  tab.scrollIntoView() tab.click() await new Promise(r  setTimeout(r, 500))  )()  crawlerconfig  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), jscodejsclicktabs, ) async with AsyncWebCrawler(configbrowserconfig) as crawler: result  await crawler.arun( urlhttps:www.kidocode.comdegreestechnology, configcrawlerconfig ) companies  json.loads(result.extractedcontent) print(fSuccessfully extracted len(companies) companies) print(json.dumps(companies0, indent2)) async def main(): await extractstructureddatausingcssextractor() if name  main: asyncio.run(main()) Search Type to start searching Search Type to start searching Search Type to start searching Search Type to start searching

---

## 文档 3 <span id='doc-3'></span>

### 元数据

- **来源URL**: https://crawl4ai.com/mkdocs/core/installation/
- **字数统计**: 15778
- **抓取时间**: 2025-02-13 17:30:59

### 内容

> Installation  Setup (2023 Edition) 1. Basic Installation pipinstallcrawl4ai This installs the core Crawl4AI library along with essential dependencies. No advanced features (like transformers or PyTorch) are included yet. 2. Initial Setup  Diagnostics 2.1 Run the Setup Command After installing, call: crawl4aisetup What does it do?  Installs or updates required Playwright browsers (Chromium, Firefox, etc.)  Performs OSlevel checks (e.g., missing libs on Linux)  Confirms your environment is ready to crawl 2.2 Diagnostics Optionally, you can run diagnostics to confirm everything is functioning: crawl4aidoctor This command attempts to:  Check Python version compatibility  Verify Playwright installation  Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and rerun crawl4aisetup. 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4aidoctor) Below is a minimal Python script demonstrating a basic crawl. It uses our new BrowserConfig and CrawlerRunConfig for clarity, though no custom settings are passed in this example: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlhttps:www.example.com, ) print(result.markdown:300)  Show the first 300 characters of extracted text if name  main: asyncio.run(main()) Expected outcome:  A headless browser session loads example.com  Crawl4AI returns 300 characters of markdown. If errors occur, rerun crawl4aidoctor or manually ensure Playwright is installed correctly. 4. Advanced Installation (Optional) Warning: Only install these if you truly need them. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly. 4.1 Torch, Transformers, or All Text Clustering (Torch) pipinstallcrawl4aitorch crawl4aisetup Installs PyTorchbased features (e.g., cosine similarity or advanced semantic chunking). Transformers pipinstallcrawl4aitransformer crawl4aisetup Adds Hugging Facebased summarization or generation strategies. All Features pipinstallcrawl4aiall crawl4aisetup (Optional) PreFetching Models crawl4aidownloadmodels This step caches large models locally (if needed). Only do this if your workflow requires them. 5. Docker (Experimental) We provide a temporary Docker approach for testing. Its not stable and may break with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try: dockerpullunclecodecrawl4ai:basic dockerrunp11235:11235unclecodecrawl4ai:basic You can then make POST requests to http:localhost:11235crawl to perform crawls. Production usage is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025). 6. Local Server Mode (Legacy) Some older docs mention running Crawl4AI as a local server. This approach has been partially replaced by the new Dockerbased prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized. Summary 1. Install with pip install crawl4ai and run crawl4aisetup. 2. Diagnose with crawl4aidoctor if you see errors. 3. Verify by crawling example.com with minimal BrowserConfig  CrawlerRunConfig. 4. Advanced features (Torch, Transformers) are optionalavoid them if you dont need them (they significantly increase resource usage). 5. Docker is experimentaluse at your own risk until the stable version is released. 6. Local server references in older docs are largely deprecated a new solution is in progress. Got questions? Check GitHub issues for updates or ask the community! Installation  Setup (2023 Edition) 1. Basic Installation pipinstallcrawl4ai This installs the core Crawl4AI library along with essential dependencies. No advanced features (like transformers or PyTorch) are included yet. 2. Initial Setup  Diagnostics 2.1 Run the Setup Command After installing, call: crawl4aisetup What does it do?  Installs or updates required Playwright browsers (Chromium, Firefox, etc.)  Performs OSlevel checks (e.g., missing libs on Linux)  Confirms your environment is ready to crawl 2.2 Diagnostics Optionally, you can run diagnostics to confirm everything is functioning: crawl4aidoctor This command attempts to:  Check Python version compatibility  Verify Playwright installation  Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and rerun crawl4aisetup. 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4aidoctor) Below is a minimal Python script demonstrating a basic crawl. It uses our new BrowserConfig and CrawlerRunConfig for clarity, though no custom settings are passed in this example: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlhttps:www.example.com, ) print(result.markdown:300)  Show the first 300 characters of extracted text if name  main: asyncio.run(main()) Expected outcome:  A headless browser session loads example.com  Crawl4AI returns 300 characters of markdown. If errors occur, rerun crawl4aidoctor or manually ensure Playwright is installed correctly. 4. Advanced Installation (Optional) Warning: Only install these if you truly need them. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly. 4.1 Torch, Transformers, or All Text Clustering (Torch) pipinstallcrawl4aitorch crawl4aisetup Installs PyTorchbased features (e.g., cosine similarity or advanced semantic chunking). Transformers pipinstallcrawl4aitransformer crawl4aisetup Adds Hugging Facebased summarization or generation strategies. All Features pipinstallcrawl4aiall crawl4aisetup (Optional) PreFetching Models crawl4aidownloadmodels This step caches large models locally (if needed). Only do this if your workflow requires them. 5. Docker (Experimental) We provide a temporary Docker approach for testing. Its not stable and may break with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try: dockerpullunclecodecrawl4ai:basic dockerrunp11235:11235unclecodecrawl4ai:basic You can then make POST requests to http:localhost:11235crawl to perform crawls. Production usage is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025). 6. Local Server Mode (Legacy) Some older docs mention running Crawl4AI as a local server. This approach has been partially replaced by the new Dockerbased prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized. Summary 1. Install with pip install crawl4ai and run crawl4aisetup. 2. Diagnose with crawl4aidoctor if you see errors. 3. Verify by crawling example.com with minimal BrowserConfig  CrawlerRunConfig. 4. Advanced features (Torch, Transformers) are optionalavoid them if you dont need them (they significantly increase resource usage). 5. Docker is experimentaluse at your own risk until the stable version is released. 6. Local server references in older docs are largely deprecated a new solution is in progress. Got questions? Check GitHub issues for updates or ask the community! Installation  Setup (2023 Edition) 1. Basic Installation pipinstallcrawl4ai This installs the core Crawl4AI library along with essential dependencies. No advanced features (like transformers or PyTorch) are included yet. 2. Initial Setup  Diagnostics 2.1 Run the Setup Command After installing, call: crawl4aisetup What does it do?  Installs or updates required Playwright browsers (Chromium, Firefox, etc.)  Performs OSlevel checks (e.g., missing libs on Linux)  Confirms your environment is ready to crawl 2.2 Diagnostics Optionally, you can run diagnostics to confirm everything is functioning: crawl4aidoctor This command attempts to:  Check Python version compatibility  Verify Playwright installation  Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and rerun crawl4aisetup. 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4aidoctor) Below is a minimal Python script demonstrating a basic crawl. It uses our new BrowserConfig and CrawlerRunConfig for clarity, though no custom settings are passed in this example: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlhttps:www.example.com, ) print(result.markdown:300)  Show the first 300 characters of extracted text if name  main: asyncio.run(main()) Expected outcome:  A headless browser session loads example.com  Crawl4AI returns 300 characters of markdown. If errors occur, rerun crawl4aidoctor or manually ensure Playwright is installed correctly. 4. Advanced Installation (Optional) Warning: Only install these if you truly need them. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly. 4.1 Torch, Transformers, or All Text Clustering (Torch) pipinstallcrawl4aitorch crawl4aisetup Installs PyTorchbased features (e.g., cosine similarity or advanced semantic chunking). Transformers pipinstallcrawl4aitransformer crawl4aisetup Adds Hugging Facebased summarization or generation strategies. All Features pipinstallcrawl4aiall crawl4aisetup (Optional) PreFetching Models crawl4aidownloadmodels This step caches large models locally (if needed). Only do this if your workflow requires them. 5. Docker (Experimental) We provide a temporary Docker approach for testing. Its not stable and may break with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try: dockerpullunclecodecrawl4ai:basic dockerrunp11235:11235unclecodecrawl4ai:basic You can then make POST requests to http:localhost:11235crawl to perform crawls. Production usage is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025). 6. Local Server Mode (Legacy) Some older docs mention running Crawl4AI as a local server. This approach has been partially replaced by the new Dockerbased prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized. Summary 1. Install with pip install crawl4ai and run crawl4aisetup. 2. Diagnose with crawl4aidoctor if you see errors. 3. Verify by crawling example.com with minimal BrowserConfig  CrawlerRunConfig. 4. Advanced features (Torch, Transformers) are optionalavoid them if you dont need them (they significantly increase resource usage). 5. Docker is experimentaluse at your own risk until the stable version is released. 6. Local server references in older docs are largely deprecated a new solution is in progress. Got questions? Check GitHub issues for updates or ask the community! Installation  Setup (2023 Edition) 1. Basic Installation pipinstallcrawl4ai This installs the core Crawl4AI library along with essential dependencies. No advanced features (like transformers or PyTorch) are included yet. 2. Initial Setup  Diagnostics 2.1 Run the Setup Command After installing, call: crawl4aisetup What does it do?  Installs or updates required Playwright browsers (Chromium, Firefox, etc.)  Performs OSlevel checks (e.g., missing libs on Linux)  Confirms your environment is ready to crawl 2.2 Diagnostics Optionally, you can run diagnostics to confirm everything is functioning: crawl4aidoctor This command attempts to:  Check Python version compatibility  Verify Playwright installation  Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and rerun crawl4aisetup. 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4aidoctor) Below is a minimal Python script demonstrating a basic crawl. It uses our new BrowserConfig and CrawlerRunConfig for clarity, though no custom settings are passed in this example: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlhttps:www.example.com, ) print(result.markdown:300)  Show the first 300 characters of extracted text if name  main: asyncio.run(main()) Expected outcome:  A headless browser session loads example.com  Crawl4AI returns 300 characters of markdown. If errors occur, rerun crawl4aidoctor or manually ensure Playwright is installed correctly. 4. Advanced Installation (Optional) Warning: Only install these if you truly need them. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly. 4.1 Torch, Transformers, or All Text Clustering (Torch) pipinstallcrawl4aitorch crawl4aisetup Installs PyTorchbased features (e.g., cosine similarity or advanced semantic chunking). Transformers pipinstallcrawl4aitransformer crawl4aisetup Adds Hugging Facebased summarization or generation strategies. All Features pipinstallcrawl4aiall crawl4aisetup (Optional) PreFetching Models crawl4aidownloadmodels This step caches large models locally (if needed). Only do this if your workflow requires them. 5. Docker (Experimental) We provide a temporary Docker approach for testing. Its not stable and may break with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try: dockerpullunclecodecrawl4ai:basic dockerrunp11235:11235unclecodecrawl4ai:basic You can then make POST requests to http:localhost:11235crawl to perform crawls. Production usage is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025). 6. Local Server Mode (Legacy) Some older docs mention running Crawl4AI as a local server. This approach has been partially replaced by the new Dockerbased prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized. Summary 1. Install with pip install crawl4ai and run crawl4aisetup. 2. Diagnose with crawl4aidoctor if you see errors. 3. Verify by crawling example.com with minimal BrowserConfig  CrawlerRunConfig. 4. Advanced features (Torch, Transformers) are optionalavoid them if you dont need them (they significantly increase resource usage). 5. Docker is experimentaluse at your own risk until the stable version is released. 6. Local server references in older docs are largely deprecated a new solution is in progress. Got questions? Check GitHub issues for updates or ask the community! pipinstallcrawl4ai crawl4aisetup crawl4aidoctor import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result  await crawler.arun( urlhttps:www.example.com, ) print(result.markdown:300)  Show the first 300 characters of extracted text if name  main: asyncio.run(main()) pipinstallcrawl4aitorch crawl4aisetup pipinstallcrawl4aitransformer crawl4aisetup pipinstallcrawl4aiall crawl4aisetup crawl4aidownloadmodels dockerpullunclecodecrawl4ai:basic dockerrunp11235:11235unclecodecrawl4ai:basic Search Type to start searching Search Type to start searching Search Type to start searching Search Type to start searching

---

## 文档 4 <span id='doc-4'></span>

### 元数据

- **来源URL**: https://crawl4ai.com/mkdocs/api/async-webcrawler/
- **字数统计**: 37834
- **抓取时间**: 2025-02-13 17:30:59

### 内容

> AsyncWebCrawler The AsyncWebCrawler is the core class for asynchronous web crawling in Crawl4AI. You typically create it once, optionally customize it with a BrowserConfig (e.g., headless, user agent), then run multiple arun() calls with different CrawlerRunConfig objects. Recommended usage: 1. Create a BrowserConfig for global browser settings. 2. Instantiate AsyncWebCrawler(configbrowserconfig). 3. Use the crawler in an async context manager (async with) or manage startclose manually. 4. Call arun(url, configcrawlerrunconfig) for each page you want. 1. Constructor Overview class AsyncWebCrawler: def init( self, crawlerstrategy: OptionalAsyncCrawlerStrategy  None, config: OptionalBrowserConfig  None, alwaysbypasscache: bool  False,  deprecated alwaysbypasscache: Optionalbool  None,  also deprecated basedirectory: str  ., threadsafe: bool  False, kwargs, ):  Create an AsyncWebCrawler instance. Args: crawlerstrategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. alwaysbypasscache: (Deprecated) Use CrawlerRunConfig.cachemode instead. basedirectory: Folder for storing cacheslogs (if relevant). threadsafe: If True, attempts some concurrency safeguards. Usually False. kwargs: Additional legacy or debugging parameters.  )  Typical Initialization python from crawl4ai import AsyncWebCrawler, BrowserConfig browsercfg  BrowserConfig( browsertypechromium, headlessTrue, verboseTrue ) crawler  AsyncWebCrawler(configbrowsercfg) Notes: Legacy parameters like alwaysbypasscache remain for backward compatibility, but prefer to set caching in CrawlerRunConfig. 2. Lifecycle: StartClose or Context Manager 2.1 Context Manager (Recommended) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.com)  The crawler automatically startscloses resources When the async with block ends, the crawler cleans up (closes the browser, etc.). 2.2 Manual Start  Close crawler  AsyncWebCrawler(configbrowsercfg) await crawler.start() result1  await crawler.arun(https:example.com) result2  await crawler.arun(https:another.com) await crawler.close() Use this style if you have a longrunning application or need full control of the crawlers lifecycle. 3. Primary Method: arun() async def arun( self, url: str, config: OptionalCrawlerRunConfig  None,  Legacy parameters for backward compatibility. )  CrawlResult: . 3.1 New Approach You pass a CrawlerRunConfig object that sets up everything about a crawlcontent filtering, caching, session reuse, JS code, screenshots, etc. import asyncio from crawl4ai import CrawlerRunConfig, CacheMode runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, cssselectormain.article, wordcountthreshold10, screenshotTrue ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.comnews, configruncfg) print(Crawled HTML length:, len(result.cleanedhtml)) if result.screenshot: print(Screenshot base64 length:, len(result.screenshot)) 3.2 Legacy Parameters Still Accepted For backward compatibility, arun() can still accept direct arguments like cssselector., wordcountthreshold., etc., but we strongly advise migrating them into a CrawlerRunConfig. 4. Batch Processing: arunmany() async def arunmany( self, urls: Liststr, config: OptionalCrawlerRunConfig  None,  Legacy parameters maintained for backwards compatibility. )  ListCrawlResult:  Process multiple URLs with intelligent rate limiting and resource monitoring.  4.1 ResourceAware Crawling The arunmany() method now uses an intelligent dispatcher that: Monitors system memory usage Implements adaptive rate limiting Provides detailed progress monitoring Manages concurrent crawls efficiently 4.2 Example Usage from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode  Configure browser browsercfg  BrowserConfig(headlessTrue)  Configure crawler with rate limiting runcfg  CrawlerRunConfig(  Enable rate limiting enableratelimitingTrue, ratelimitconfigRateLimitConfig( basedelay(1.0, 2.0),  Random delay between 12 seconds maxdelay30.0,  Maximum delay after rate limit hits maxretries2,  Number of retries before giving up ratelimitcodes429, 503  Status codes that trigger rate limiting ),  Resource monitoring memorythresholdpercent70.0,  Pause if memory exceeds this checkinterval0.5,  How often to check resources maxsessionpermit3,  Maximum concurrent crawls displaymodeDisplayMode.DETAILED.value  Show detailed progress ) urls   https:example.compage1, https:example.compage2, https:example.compage3  async with AsyncWebCrawler(configbrowsercfg) as crawler: results  await crawler.arunmany(urls, configruncfg) for result in results: print(fURL: result.url, Success: result.success) 4.3 Key Features 1. Rate Limiting Automatic delay between requests Exponential backoff on rate limit detection Domainspecific rate limiting Configurable retry strategy 2. Resource Monitoring Memory usage tracking Adaptive concurrency based on system load Automatic pausing when resources are constrained 3. Progress Monitoring Detailed or aggregated progress display Realtime status updates Memory usage statistics 4. Error Handling Graceful handling of rate limits Automatic retries with backoff Detailed error reporting 5. CrawlResult Output Each arun() returns a CrawlResult containing: url: Final URL (if redirected). html: Original HTML. cleanedhtml: Sanitized HTML. markdownv2 (or future markdown): Markdown outputs (raw, fit, etc.). extractedcontent: If an extraction strategy was used (JSON for CSSLLM strategies). screenshot, pdf: If screenshotsPDF requested. media, links: Information about discovered imageslinks. success, errormessage: Status info. For details, see CrawlResult doc. 6. Quick Example Below is an example hooking it all together: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy import json async def main():  1. Browser config browsercfg  BrowserConfig( browsertypefirefox, headlessFalse, verboseTrue )  2. Run config schema   name: Articles, baseSelector: article.post, fields:   name: title, selector: h2, type: text ,  name: url, selector: a, type: attribute, attribute: href    runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), wordcountthreshold15, removeoverlayelementsTrue, waitforcss:.post  Wait for posts to appear ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun( urlhttps:example.comblog, configruncfg ) if result.success: print(Cleaned HTML length:, len(result.cleanedhtml)) if result.extractedcontent: articles  json.loads(result.extractedcontent) print(Extracted articles:, articles:2) else: print(Error:, result.errormessage) asyncio.run(main()) Explanation: We define a BrowserConfig with Firefox, no headless, and verboseTrue. We define a CrawlerRunConfig that bypasses cache, uses a CSS extraction schema, has a wordcountthreshold15, etc. We pass them to AsyncWebCrawler(config.) and arun(url., config.). 7. Best Practices  Migration Notes 1. Use BrowserConfig for global settings about the browsers environment. 2. Use CrawlerRunConfig for percrawl logic (caching, content filtering, extraction strategies, wait conditions). 3. Avoid legacy parameters like cssselector or wordcountthreshold directly in arun(). Instead: runcfg  CrawlerRunConfig(cssselector.maincontent, wordcountthreshold20) result  await crawler.arun(url., configruncfg) 4. Context Manager usage is simplest unless you want a persistent crawler across many calls. 8. Summary AsyncWebCrawler is your entry point to asynchronous crawling: Constructor accepts BrowserConfig (or defaults). arun(url, configCrawlerRunConfig) is the main method for singlepage crawls. arunmany(urls, configCrawlerRunConfig) handles concurrency across multiple URLs. For advanced lifecycle control, use start() and close() explicitly. Migration: If you used AsyncWebCrawler(browsertypechromium, cssselector.), move browser settings to BrowserConfig(.) and contentcrawl logic to CrawlerRunConfig(.). This modular approach ensures your code is clean, scalable, and easy to maintain. For any advanced or rarely used parameters, see the BrowserConfig docs. AsyncWebCrawler The AsyncWebCrawler is the core class for asynchronous web crawling in Crawl4AI. You typically create it once, optionally customize it with a BrowserConfig (e.g., headless, user agent), then run multiple arun() calls with different CrawlerRunConfig objects. Recommended usage: 1. Create a BrowserConfig for global browser settings. 2. Instantiate AsyncWebCrawler(configbrowserconfig). 3. Use the crawler in an async context manager (async with) or manage startclose manually. 4. Call arun(url, configcrawlerrunconfig) for each page you want. 1. Constructor Overview class AsyncWebCrawler: def init( self, crawlerstrategy: OptionalAsyncCrawlerStrategy  None, config: OptionalBrowserConfig  None, alwaysbypasscache: bool  False,  deprecated alwaysbypasscache: Optionalbool  None,  also deprecated basedirectory: str  ., threadsafe: bool  False, kwargs, ):  Create an AsyncWebCrawler instance. Args: crawlerstrategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. alwaysbypasscache: (Deprecated) Use CrawlerRunConfig.cachemode instead. basedirectory: Folder for storing cacheslogs (if relevant). threadsafe: If True, attempts some concurrency safeguards. Usually False. kwargs: Additional legacy or debugging parameters.  )  Typical Initialization python from crawl4ai import AsyncWebCrawler, BrowserConfig browsercfg  BrowserConfig( browsertypechromium, headlessTrue, verboseTrue ) crawler  AsyncWebCrawler(configbrowsercfg) Notes: Legacy parameters like alwaysbypasscache remain for backward compatibility, but prefer to set caching in CrawlerRunConfig. 2. Lifecycle: StartClose or Context Manager 2.1 Context Manager (Recommended) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.com)  The crawler automatically startscloses resources When the async with block ends, the crawler cleans up (closes the browser, etc.). 2.2 Manual Start  Close crawler  AsyncWebCrawler(configbrowsercfg) await crawler.start() result1  await crawler.arun(https:example.com) result2  await crawler.arun(https:another.com) await crawler.close() Use this style if you have a longrunning application or need full control of the crawlers lifecycle. 3. Primary Method: arun() async def arun( self, url: str, config: OptionalCrawlerRunConfig  None,  Legacy parameters for backward compatibility. )  CrawlResult: . 3.1 New Approach You pass a CrawlerRunConfig object that sets up everything about a crawlcontent filtering, caching, session reuse, JS code, screenshots, etc. import asyncio from crawl4ai import CrawlerRunConfig, CacheMode runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, cssselectormain.article, wordcountthreshold10, screenshotTrue ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.comnews, configruncfg) print(Crawled HTML length:, len(result.cleanedhtml)) if result.screenshot: print(Screenshot base64 length:, len(result.screenshot)) 3.2 Legacy Parameters Still Accepted For backward compatibility, arun() can still accept direct arguments like cssselector., wordcountthreshold., etc., but we strongly advise migrating them into a CrawlerRunConfig. 4. Batch Processing: arunmany() async def arunmany( self, urls: Liststr, config: OptionalCrawlerRunConfig  None,  Legacy parameters maintained for backwards compatibility. )  ListCrawlResult:  Process multiple URLs with intelligent rate limiting and resource monitoring.  4.1 ResourceAware Crawling The arunmany() method now uses an intelligent dispatcher that: Monitors system memory usage Implements adaptive rate limiting Provides detailed progress monitoring Manages concurrent crawls efficiently 4.2 Example Usage from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode  Configure browser browsercfg  BrowserConfig(headlessTrue)  Configure crawler with rate limiting runcfg  CrawlerRunConfig(  Enable rate limiting enableratelimitingTrue, ratelimitconfigRateLimitConfig( basedelay(1.0, 2.0),  Random delay between 12 seconds maxdelay30.0,  Maximum delay after rate limit hits maxretries2,  Number of retries before giving up ratelimitcodes429, 503  Status codes that trigger rate limiting ),  Resource monitoring memorythresholdpercent70.0,  Pause if memory exceeds this checkinterval0.5,  How often to check resources maxsessionpermit3,  Maximum concurrent crawls displaymodeDisplayMode.DETAILED.value  Show detailed progress ) urls   https:example.compage1, https:example.compage2, https:example.compage3  async with AsyncWebCrawler(configbrowsercfg) as crawler: results  await crawler.arunmany(urls, configruncfg) for result in results: print(fURL: result.url, Success: result.success) 4.3 Key Features 1. Rate Limiting Automatic delay between requests Exponential backoff on rate limit detection Domainspecific rate limiting Configurable retry strategy 2. Resource Monitoring Memory usage tracking Adaptive concurrency based on system load Automatic pausing when resources are constrained 3. Progress Monitoring Detailed or aggregated progress display Realtime status updates Memory usage statistics 4. Error Handling Graceful handling of rate limits Automatic retries with backoff Detailed error reporting 5. CrawlResult Output Each arun() returns a CrawlResult containing: url: Final URL (if redirected). html: Original HTML. cleanedhtml: Sanitized HTML. markdownv2 (or future markdown): Markdown outputs (raw, fit, etc.). extractedcontent: If an extraction strategy was used (JSON for CSSLLM strategies). screenshot, pdf: If screenshotsPDF requested. media, links: Information about discovered imageslinks. success, errormessage: Status info. For details, see CrawlResult doc. 6. Quick Example Below is an example hooking it all together: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy import json async def main():  1. Browser config browsercfg  BrowserConfig( browsertypefirefox, headlessFalse, verboseTrue )  2. Run config schema   name: Articles, baseSelector: article.post, fields:   name: title, selector: h2, type: text ,  name: url, selector: a, type: attribute, attribute: href    runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), wordcountthreshold15, removeoverlayelementsTrue, waitforcss:.post  Wait for posts to appear ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun( urlhttps:example.comblog, configruncfg ) if result.success: print(Cleaned HTML length:, len(result.cleanedhtml)) if result.extractedcontent: articles  json.loads(result.extractedcontent) print(Extracted articles:, articles:2) else: print(Error:, result.errormessage) asyncio.run(main()) Explanation: We define a BrowserConfig with Firefox, no headless, and verboseTrue. We define a CrawlerRunConfig that bypasses cache, uses a CSS extraction schema, has a wordcountthreshold15, etc. We pass them to AsyncWebCrawler(config.) and arun(url., config.). 7. Best Practices  Migration Notes 1. Use BrowserConfig for global settings about the browsers environment. 2. Use CrawlerRunConfig for percrawl logic (caching, content filtering, extraction strategies, wait conditions). 3. Avoid legacy parameters like cssselector or wordcountthreshold directly in arun(). Instead: runcfg  CrawlerRunConfig(cssselector.maincontent, wordcountthreshold20) result  await crawler.arun(url., configruncfg) 4. Context Manager usage is simplest unless you want a persistent crawler across many calls. 8. Summary AsyncWebCrawler is your entry point to asynchronous crawling: Constructor accepts BrowserConfig (or defaults). arun(url, configCrawlerRunConfig) is the main method for singlepage crawls. arunmany(urls, configCrawlerRunConfig) handles concurrency across multiple URLs. For advanced lifecycle control, use start() and close() explicitly. Migration: If you used AsyncWebCrawler(browsertypechromium, cssselector.), move browser settings to BrowserConfig(.) and contentcrawl logic to CrawlerRunConfig(.). This modular approach ensures your code is clean, scalable, and easy to maintain. For any advanced or rarely used parameters, see the BrowserConfig docs. AsyncWebCrawler The AsyncWebCrawler is the core class for asynchronous web crawling in Crawl4AI. You typically create it once, optionally customize it with a BrowserConfig (e.g., headless, user agent), then run multiple arun() calls with different CrawlerRunConfig objects. Recommended usage: 1. Create a BrowserConfig for global browser settings. 2. Instantiate AsyncWebCrawler(configbrowserconfig). 3. Use the crawler in an async context manager (async with) or manage startclose manually. 4. Call arun(url, configcrawlerrunconfig) for each page you want. 1. Constructor Overview class AsyncWebCrawler: def init( self, crawlerstrategy: OptionalAsyncCrawlerStrategy  None, config: OptionalBrowserConfig  None, alwaysbypasscache: bool  False,  deprecated alwaysbypasscache: Optionalbool  None,  also deprecated basedirectory: str  ., threadsafe: bool  False, kwargs, ):  Create an AsyncWebCrawler instance. Args: crawlerstrategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. alwaysbypasscache: (Deprecated) Use CrawlerRunConfig.cachemode instead. basedirectory: Folder for storing cacheslogs (if relevant). threadsafe: If True, attempts some concurrency safeguards. Usually False. kwargs: Additional legacy or debugging parameters.  )  Typical Initialization python from crawl4ai import AsyncWebCrawler, BrowserConfig browsercfg  BrowserConfig( browsertypechromium, headlessTrue, verboseTrue ) crawler  AsyncWebCrawler(configbrowsercfg) Notes: Legacy parameters like alwaysbypasscache remain for backward compatibility, but prefer to set caching in CrawlerRunConfig. 2. Lifecycle: StartClose or Context Manager 2.1 Context Manager (Recommended) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.com)  The crawler automatically startscloses resources When the async with block ends, the crawler cleans up (closes the browser, etc.). 2.2 Manual Start  Close crawler  AsyncWebCrawler(configbrowsercfg) await crawler.start() result1  await crawler.arun(https:example.com) result2  await crawler.arun(https:another.com) await crawler.close() Use this style if you have a longrunning application or need full control of the crawlers lifecycle. 3. Primary Method: arun() async def arun( self, url: str, config: OptionalCrawlerRunConfig  None,  Legacy parameters for backward compatibility. )  CrawlResult: . 3.1 New Approach You pass a CrawlerRunConfig object that sets up everything about a crawlcontent filtering, caching, session reuse, JS code, screenshots, etc. import asyncio from crawl4ai import CrawlerRunConfig, CacheMode runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, cssselectormain.article, wordcountthreshold10, screenshotTrue ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.comnews, configruncfg) print(Crawled HTML length:, len(result.cleanedhtml)) if result.screenshot: print(Screenshot base64 length:, len(result.screenshot)) 3.2 Legacy Parameters Still Accepted For backward compatibility, arun() can still accept direct arguments like cssselector., wordcountthreshold., etc., but we strongly advise migrating them into a CrawlerRunConfig. 4. Batch Processing: arunmany() async def arunmany( self, urls: Liststr, config: OptionalCrawlerRunConfig  None,  Legacy parameters maintained for backwards compatibility. )  ListCrawlResult:  Process multiple URLs with intelligent rate limiting and resource monitoring.  4.1 ResourceAware Crawling The arunmany() method now uses an intelligent dispatcher that: Monitors system memory usage Implements adaptive rate limiting Provides detailed progress monitoring Manages concurrent crawls efficiently 4.2 Example Usage from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode  Configure browser browsercfg  BrowserConfig(headlessTrue)  Configure crawler with rate limiting runcfg  CrawlerRunConfig(  Enable rate limiting enableratelimitingTrue, ratelimitconfigRateLimitConfig( basedelay(1.0, 2.0),  Random delay between 12 seconds maxdelay30.0,  Maximum delay after rate limit hits maxretries2,  Number of retries before giving up ratelimitcodes429, 503  Status codes that trigger rate limiting ),  Resource monitoring memorythresholdpercent70.0,  Pause if memory exceeds this checkinterval0.5,  How often to check resources maxsessionpermit3,  Maximum concurrent crawls displaymodeDisplayMode.DETAILED.value  Show detailed progress ) urls   https:example.compage1, https:example.compage2, https:example.compage3  async with AsyncWebCrawler(configbrowsercfg) as crawler: results  await crawler.arunmany(urls, configruncfg) for result in results: print(fURL: result.url, Success: result.success) 4.3 Key Features 1. Rate Limiting Automatic delay between requests Exponential backoff on rate limit detection Domainspecific rate limiting Configurable retry strategy 2. Resource Monitoring Memory usage tracking Adaptive concurrency based on system load Automatic pausing when resources are constrained 3. Progress Monitoring Detailed or aggregated progress display Realtime status updates Memory usage statistics 4. Error Handling Graceful handling of rate limits Automatic retries with backoff Detailed error reporting 5. CrawlResult Output Each arun() returns a CrawlResult containing: url: Final URL (if redirected). html: Original HTML. cleanedhtml: Sanitized HTML. markdownv2 (or future markdown): Markdown outputs (raw, fit, etc.). extractedcontent: If an extraction strategy was used (JSON for CSSLLM strategies). screenshot, pdf: If screenshotsPDF requested. media, links: Information about discovered imageslinks. success, errormessage: Status info. For details, see CrawlResult doc. 6. Quick Example Below is an example hooking it all together: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy import json async def main():  1. Browser config browsercfg  BrowserConfig( browsertypefirefox, headlessFalse, verboseTrue )  2. Run config schema   name: Articles, baseSelector: article.post, fields:   name: title, selector: h2, type: text ,  name: url, selector: a, type: attribute, attribute: href    runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), wordcountthreshold15, removeoverlayelementsTrue, waitforcss:.post  Wait for posts to appear ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun( urlhttps:example.comblog, configruncfg ) if result.success: print(Cleaned HTML length:, len(result.cleanedhtml)) if result.extractedcontent: articles  json.loads(result.extractedcontent) print(Extracted articles:, articles:2) else: print(Error:, result.errormessage) asyncio.run(main()) Explanation: We define a BrowserConfig with Firefox, no headless, and verboseTrue. We define a CrawlerRunConfig that bypasses cache, uses a CSS extraction schema, has a wordcountthreshold15, etc. We pass them to AsyncWebCrawler(config.) and arun(url., config.). 7. Best Practices  Migration Notes 1. Use BrowserConfig for global settings about the browsers environment. 2. Use CrawlerRunConfig for percrawl logic (caching, content filtering, extraction strategies, wait conditions). 3. Avoid legacy parameters like cssselector or wordcountthreshold directly in arun(). Instead: runcfg  CrawlerRunConfig(cssselector.maincontent, wordcountthreshold20) result  await crawler.arun(url., configruncfg) 4. Context Manager usage is simplest unless you want a persistent crawler across many calls. 8. Summary AsyncWebCrawler is your entry point to asynchronous crawling: Constructor accepts BrowserConfig (or defaults). arun(url, configCrawlerRunConfig) is the main method for singlepage crawls. arunmany(urls, configCrawlerRunConfig) handles concurrency across multiple URLs. For advanced lifecycle control, use start() and close() explicitly. Migration: If you used AsyncWebCrawler(browsertypechromium, cssselector.), move browser settings to BrowserConfig(.) and contentcrawl logic to CrawlerRunConfig(.). This modular approach ensures your code is clean, scalable, and easy to maintain. For any advanced or rarely used parameters, see the BrowserConfig docs. AsyncWebCrawler The AsyncWebCrawler is the core class for asynchronous web crawling in Crawl4AI. You typically create it once, optionally customize it with a BrowserConfig (e.g., headless, user agent), then run multiple arun() calls with different CrawlerRunConfig objects. Recommended usage: 1. Create a BrowserConfig for global browser settings. 2. Instantiate AsyncWebCrawler(configbrowserconfig). 3. Use the crawler in an async context manager (async with) or manage startclose manually. 4. Call arun(url, configcrawlerrunconfig) for each page you want. 1. Constructor Overview class AsyncWebCrawler: def init( self, crawlerstrategy: OptionalAsyncCrawlerStrategy  None, config: OptionalBrowserConfig  None, alwaysbypasscache: bool  False,  deprecated alwaysbypasscache: Optionalbool  None,  also deprecated basedirectory: str  ., threadsafe: bool  False, kwargs, ):  Create an AsyncWebCrawler instance. Args: crawlerstrategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. alwaysbypasscache: (Deprecated) Use CrawlerRunConfig.cachemode instead. basedirectory: Folder for storing cacheslogs (if relevant). threadsafe: If True, attempts some concurrency safeguards. Usually False. kwargs: Additional legacy or debugging parameters.  )  Typical Initialization python from crawl4ai import AsyncWebCrawler, BrowserConfig browsercfg  BrowserConfig( browsertypechromium, headlessTrue, verboseTrue ) crawler  AsyncWebCrawler(configbrowsercfg) Notes: Legacy parameters like alwaysbypasscache remain for backward compatibility, but prefer to set caching in CrawlerRunConfig. 2. Lifecycle: StartClose or Context Manager 2.1 Context Manager (Recommended) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.com)  The crawler automatically startscloses resources When the async with block ends, the crawler cleans up (closes the browser, etc.). 2.2 Manual Start  Close crawler  AsyncWebCrawler(configbrowsercfg) await crawler.start() result1  await crawler.arun(https:example.com) result2  await crawler.arun(https:another.com) await crawler.close() Use this style if you have a longrunning application or need full control of the crawlers lifecycle. 3. Primary Method: arun() async def arun( self, url: str, config: OptionalCrawlerRunConfig  None,  Legacy parameters for backward compatibility. )  CrawlResult: . 3.1 New Approach You pass a CrawlerRunConfig object that sets up everything about a crawlcontent filtering, caching, session reuse, JS code, screenshots, etc. import asyncio from crawl4ai import CrawlerRunConfig, CacheMode runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, cssselectormain.article, wordcountthreshold10, screenshotTrue ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.comnews, configruncfg) print(Crawled HTML length:, len(result.cleanedhtml)) if result.screenshot: print(Screenshot base64 length:, len(result.screenshot)) 3.2 Legacy Parameters Still Accepted For backward compatibility, arun() can still accept direct arguments like cssselector., wordcountthreshold., etc., but we strongly advise migrating them into a CrawlerRunConfig. 4. Batch Processing: arunmany() async def arunmany( self, urls: Liststr, config: OptionalCrawlerRunConfig  None,  Legacy parameters maintained for backwards compatibility. )  ListCrawlResult:  Process multiple URLs with intelligent rate limiting and resource monitoring.  4.1 ResourceAware Crawling The arunmany() method now uses an intelligent dispatcher that: Monitors system memory usage Implements adaptive rate limiting Provides detailed progress monitoring Manages concurrent crawls efficiently 4.2 Example Usage from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode  Configure browser browsercfg  BrowserConfig(headlessTrue)  Configure crawler with rate limiting runcfg  CrawlerRunConfig(  Enable rate limiting enableratelimitingTrue, ratelimitconfigRateLimitConfig( basedelay(1.0, 2.0),  Random delay between 12 seconds maxdelay30.0,  Maximum delay after rate limit hits maxretries2,  Number of retries before giving up ratelimitcodes429, 503  Status codes that trigger rate limiting ),  Resource monitoring memorythresholdpercent70.0,  Pause if memory exceeds this checkinterval0.5,  How often to check resources maxsessionpermit3,  Maximum concurrent crawls displaymodeDisplayMode.DETAILED.value  Show detailed progress ) urls   https:example.compage1, https:example.compage2, https:example.compage3  async with AsyncWebCrawler(configbrowsercfg) as crawler: results  await crawler.arunmany(urls, configruncfg) for result in results: print(fURL: result.url, Success: result.success) 4.3 Key Features 1. Rate Limiting Automatic delay between requests Exponential backoff on rate limit detection Domainspecific rate limiting Configurable retry strategy 2. Resource Monitoring Memory usage tracking Adaptive concurrency based on system load Automatic pausing when resources are constrained 3. Progress Monitoring Detailed or aggregated progress display Realtime status updates Memory usage statistics 4. Error Handling Graceful handling of rate limits Automatic retries with backoff Detailed error reporting 5. CrawlResult Output Each arun() returns a CrawlResult containing: url: Final URL (if redirected). html: Original HTML. cleanedhtml: Sanitized HTML. markdownv2 (or future markdown): Markdown outputs (raw, fit, etc.). extractedcontent: If an extraction strategy was used (JSON for CSSLLM strategies). screenshot, pdf: If screenshotsPDF requested. media, links: Information about discovered imageslinks. success, errormessage: Status info. For details, see CrawlResult doc. 6. Quick Example Below is an example hooking it all together: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy import json async def main():  1. Browser config browsercfg  BrowserConfig( browsertypefirefox, headlessFalse, verboseTrue )  2. Run config schema   name: Articles, baseSelector: article.post, fields:   name: title, selector: h2, type: text ,  name: url, selector: a, type: attribute, attribute: href    runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), wordcountthreshold15, removeoverlayelementsTrue, waitforcss:.post  Wait for posts to appear ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun( urlhttps:example.comblog, configruncfg ) if result.success: print(Cleaned HTML length:, len(result.cleanedhtml)) if result.extractedcontent: articles  json.loads(result.extractedcontent) print(Extracted articles:, articles:2) else: print(Error:, result.errormessage) asyncio.run(main()) Explanation: We define a BrowserConfig with Firefox, no headless, and verboseTrue. We define a CrawlerRunConfig that bypasses cache, uses a CSS extraction schema, has a wordcountthreshold15, etc. We pass them to AsyncWebCrawler(config.) and arun(url., config.). 7. Best Practices  Migration Notes 1. Use BrowserConfig for global settings about the browsers environment. 2. Use CrawlerRunConfig for percrawl logic (caching, content filtering, extraction strategies, wait conditions). 3. Avoid legacy parameters like cssselector or wordcountthreshold directly in arun(). Instead: runcfg  CrawlerRunConfig(cssselector.maincontent, wordcountthreshold20) result  await crawler.arun(url., configruncfg) 4. Context Manager usage is simplest unless you want a persistent crawler across many calls. 8. Summary AsyncWebCrawler is your entry point to asynchronous crawling: Constructor accepts BrowserConfig (or defaults). arun(url, configCrawlerRunConfig) is the main method for singlepage crawls. arunmany(urls, configCrawlerRunConfig) handles concurrency across multiple URLs. For advanced lifecycle control, use start() and close() explicitly. Migration: If you used AsyncWebCrawler(browsertypechromium, cssselector.), move browser settings to BrowserConfig(.) and contentcrawl logic to CrawlerRunConfig(.). This modular approach ensures your code is clean, scalable, and easy to maintain. For any advanced or rarely used parameters, see the BrowserConfig docs. class AsyncWebCrawler: def init( self, crawlerstrategy: OptionalAsyncCrawlerStrategy  None, config: OptionalBrowserConfig  None, alwaysbypasscache: bool  False,  deprecated alwaysbypasscache: Optionalbool  None,  also deprecated basedirectory: str  ., threadsafe: bool  False, kwargs, ):  Create an AsyncWebCrawler instance. Args: crawlerstrategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. alwaysbypasscache: (Deprecated) Use CrawlerRunConfig.cachemode instead. basedirectory: Folder for storing cacheslogs (if relevant). threadsafe: If True, attempts some concurrency safeguards. Usually False. kwargs: Additional legacy or debugging parameters.  )  Typical Initialization python from crawl4ai import AsyncWebCrawler, BrowserConfig browsercfg  BrowserConfig( browsertypechromium, headlessTrue, verboseTrue ) crawler  AsyncWebCrawler(configbrowsercfg) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.com)  The crawler automatically startscloses resources crawler  AsyncWebCrawler(configbrowsercfg) await crawler.start() result1  await crawler.arun(https:example.com) result2  await crawler.arun(https:another.com) await crawler.close() async def arun( self, url: str, config: OptionalCrawlerRunConfig  None,  Legacy parameters for backward compatibility. )  CrawlResult: . import asyncio from crawl4ai import CrawlerRunConfig, CacheMode runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, cssselectormain.article, wordcountthreshold10, screenshotTrue ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun(https:example.comnews, configruncfg) print(Crawled HTML length:, len(result.cleanedhtml)) if result.screenshot: print(Screenshot base64 length:, len(result.screenshot)) async def arunmany( self, urls: Liststr, config: OptionalCrawlerRunConfig  None,  Legacy parameters maintained for backwards compatibility. )  ListCrawlResult:  Process multiple URLs with intelligent rate limiting and resource monitoring.  from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode  Configure browser browsercfg  BrowserConfig(headlessTrue)  Configure crawler with rate limiting runcfg  CrawlerRunConfig(  Enable rate limiting enableratelimitingTrue, ratelimitconfigRateLimitConfig( basedelay(1.0, 2.0),  Random delay between 12 seconds maxdelay30.0,  Maximum delay after rate limit hits maxretries2,  Number of retries before giving up ratelimitcodes429, 503  Status codes that trigger rate limiting ),  Resource monitoring memorythresholdpercent70.0,  Pause if memory exceeds this checkinterval0.5,  How often to check resources maxsessionpermit3,  Maximum concurrent crawls displaymodeDisplayMode.DETAILED.value  Show detailed progress ) urls   https:example.compage1, https:example.compage2, https:example.compage3  async with AsyncWebCrawler(configbrowsercfg) as crawler: results  await crawler.arunmany(urls, configruncfg) for result in results: print(fURL: result.url, Success: result.success) import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy import json async def main():  1. Browser config browsercfg  BrowserConfig( browsertypefirefox, headlessFalse, verboseTrue )  2. Run config schema   name: Articles, baseSelector: article.post, fields:   name: title, selector: h2, type: text ,  name: url, selector: a, type: attribute, attribute: href    runcfg  CrawlerRunConfig( cachemodeCacheMode.BYPASS, extractionstrategyJsonCssExtractionStrategy(schema), wordcountthreshold15, removeoverlayelementsTrue, waitforcss:.post  Wait for posts to appear ) async with AsyncWebCrawler(configbrowsercfg) as crawler: result  await crawler.arun( urlhttps:example.comblog, configruncfg ) if result.success: print(Cleaned HTML length:, len(result.cleanedhtml)) if result.extractedcontent: articles  json.loads(result.extractedcontent) print(Extracted articles:, articles:2) else: print(Error:, result.errormessage) asyncio.run(main()) runcfg  CrawlerRunConfig(cssselector.maincontent, wordcountthreshold20) result  await crawler.arun(url., configruncfg) Search Type to start searching Search Type to start searching Search Type to start searching Search Type to start searching

---

